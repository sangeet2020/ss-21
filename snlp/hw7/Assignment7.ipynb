{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuLYCkGygMdT"
   },
   "source": [
    "# Assignment 7\n",
    "\n",
    "Name 1: Sangeet Sagar<br/>\n",
    "Student id 1: 7009050<br/>\n",
    "Email 1: sasa00001@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: Nikhil Paliwal<br/>\n",
    "Student id 2: 7009915<br/>\n",
    "Email 2: nipa00002@stud.uni-saarland.de<br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python files for exercises 1 and 2. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after your upload your submission, otherwise the assignment will not be considered as submitted. Only one from the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import matplotlib as mpl\n",
    "seaborn.set()\n",
    "mpl.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feU_fawmgoPp"
   },
   "source": [
    "## Exercise 1: Count-Trees (5 points)\n",
    "\n",
    "Your task is to implement a count-tree with variable maximum history. It is a memory-efficient way to store n-gram counts which can also be used to create an intuitive back-off after the tree is pruned.\n",
    "\n",
    "The tree object should support the following four operations:\n",
    "- Increment a count of a specific n-gram (even if not present)\n",
    "- Retrieve counts given the history (variable length)\n",
    "- Retrieve the conditional probability of a word given the history. (Proportion count between branches)\n",
    "- Pruning all nodes with counts less or equal to $k$\n",
    "\n",
    "**1.1 (2 points)**\n",
    "\n",
    "Make sure your implementation is correct by passing the asserts in the first code cell.\n",
    "\n",
    "**1.2 (1 point)**\n",
    "\n",
    "The next cell will incrementally add a quad-gram to the tree. Plot the perplexity of trigram language model (induced by this count tree) against the number of added n-grams. Comment on the curve shape. Smooth this language model with a zerogram distribution using a linear combination ($0.75\\times p_4 + 0.25\\times p_0$).\n",
    "\n",
    "**1.3 (1 point)**\n",
    "\n",
    "For the given range of thresholds, prune your tree and see how the threshold affects the performance. Plot the results (perplexity vs. threshold).\n",
    "\n",
    "**1.4 (1 point)**\n",
    "\n",
    "1. If you first prune with threshold $k_1$ and get tree $t_1$, then prune with $k_2$ and get $t_2$ what will be the relationship between $t_1$ and $t_2$ if $k_1 \\ge k_2$? (0.25 points)\n",
    "2. What is the memory benefit of count trees, in comparison to storing the counts as a dictionary `{n-gram:freq}`? (0.25 points)\n",
    "3. If we pruned the tree so that only the first level is preserved, what distribution could we model with this tree? (0.25 points)\n",
    "4. Pruning the count tree is said to be a dynamic way of smoothing the language model. Elaborate on how this smoothing happens. (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_zKzeBzZLksw"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_1\n",
    "exercise_1 = reload(exercise_1)\n",
    "\n",
    "tree = exercise_1.CountTree(n=4)\n",
    "\n",
    "assert tree.get(\"\") == 0\n",
    "tree.add(\"ABCE\")\n",
    "tree.add(\"ABCD\")\n",
    "tree.add(\"ABCD\")\n",
    "tree.add(\"QBCD\")\n",
    "tree.add(\"QQCD\")\n",
    "tree.add(\"BCDA\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"5634\")\n",
    "assert tree.get(\"ABCD\") == 2\n",
    "assert tree.get(\"ABCX\") == 0\n",
    "assert tree.get(\"BCD\") == 3\n",
    "assert tree.get(\"D\") == 4\n",
    "assert tree.get(\"CD\") == 4\n",
    "assert tree.get(\"1234\") == 5\n",
    "assert tree.get(\"5634\") == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.prune(4)\n",
    "assert tree.get(\"ABCD\") == 4\n",
    "assert tree.get(\"XXCD\") == 4\n",
    "assert tree.get(\"D\") == 4\n",
    "assert tree.get(\"1234\") == 5\n",
    "assert tree.get(\"5634\") == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "mZQ7BgD3Xr2j",
    "outputId": "56c5573c-c789-4f1f-df4d-ba909b97fa6a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_x = []\n",
    "plot_y = []\n",
    "\n",
    "ngrams = []\n",
    "tree = exercise_1.CountTree(n=4)\n",
    "with open(\"data/alice_in_wonderland.txt\", \"r\") as f:\n",
    "  tokens = f.read().lower().split()\n",
    "  for i in range(len(tokens)-4):\n",
    "    ngrams.append(tokens[i:i+4])\n",
    "\n",
    "vocab = set(tokens)\n",
    "for i,ngram in enumerate(ngrams):\n",
    "  tree.add(ngram)\n",
    "  if i % 1000 == 0:\n",
    "    plot_x.append(i)\n",
    "    plot_y.append(tree.perplexity(ngrams, vocab))\n",
    "\n",
    "plt.plot(plot_x, plot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "QYPY_Whbooor",
    "outputId": "bb6c8e97-f6ea-41fd-e578-557fdb281ce1"
   },
   "outputs": [],
   "source": [
    "plot_x = []\n",
    "plot_y = []\n",
    "\n",
    "for threshold in [1,2,3,4,5,10,25,50,75,100]:\n",
    "  tree.prune(threshold)\n",
    "  plot_x.append(threshold)\n",
    "  plot_y.append(tree.perplexity(ngrams, vocab))\n",
    "\n",
    "print(plot_x)\n",
    "print(plot_y)\n",
    "\n",
    "plt.plot(plot_x, plot_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC_TytVNlgtr"
   },
   "source": [
    "## Exercise 2: Kneser-Ney Smoothing (5 points)\n",
    "\n",
    "This exercise aims to provide a basic understanding of Kneser-Ney Smoothing. Kneser-Ney Smoothing makes use of *continuation counts* of words for lower order n-grams, given as\n",
    "\n",
    "\\begin{equation}\n",
    "C_{KN} = \n",
    "\\begin{cases}\n",
    "\\text{count}(\\bullet) & \\text{for highest order} \\\\\n",
    "\\text{continuationcount}(\\bullet) & \\text{for lower orders}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "For a trigram distribution, Kneser-Ney Smoothing is implemented using the following equations:\n",
    "\n",
    "$$P_{KN}(w_3|w_1, w_2) = \\frac{\\max\\{N(w_1 w_2 w_3)-d,0\\}}{N(w_1 w_2)} + \\lambda(w_1, w_2)P_{KN}(w_3|w_2)$$\n",
    "\n",
    "$$P_{KN}(w_3|w_2) = \\frac{\\max\\{N_{+}(\\bullet w_2 w_3)-d,0\\}}{N_{+}(\\bullet w_2 \\bullet)} + \\lambda(w_2)P_{KN}(w_3)$$\n",
    "\n",
    "\\begin{equation}\n",
    "P_{KN}(w_3) = \\begin{cases}\n",
    "\\frac{N_{+}(\\bullet w_3)}{N_{+}(\\bullet \\bullet)} & \\text{if $w_3 \\in$ V} \\\\\n",
    "\\frac{1}{V} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "$\\lambda$ is used to normalise the discounted probability mass and is given by\n",
    "\n",
    "$$\\lambda(w_1, w_2) = \\frac{d}{N(w_1 w_2)} \\cdot N_{+}(w_1 w_2 \\bullet)$$\n",
    "\n",
    "$$\\lambda(w_2) = \\frac{d}{N(w_2)} \\cdot N_{+}(w_2 \\bullet)$$\n",
    "\n",
    "**2.1 (4.5 points)**\n",
    "\n",
    "* Your first task is to understand what these terms represent and fill it in the table below (4-5 words each).\n",
    "\n",
    "* Create a trigram-level model on the given text, `alice_in_wonderland.txt`. Write your implementation in the file `exercise_2.py`. Preprocess the text by punctuation removal, lowercasing, and tokenisation. There is no need to split the data into train and test sets. (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vNH_zLxM5amO"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_2\n",
    "exercise_2 = reload(exercise_2)\n",
    "\n",
    "file = open(\"data/alice_in_wonderland.txt\", \"r\")\n",
    "text = file.read()\n",
    "\n",
    "# TODO: Preprocess text\n",
    "tokens = exercise_2.preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vw1xFixt2rUa"
   },
   "source": [
    "* \n",
    "Write a simple class `KneserNey` in `exercise_2` that calculates the different parameters required for finding the trigram conditional probability. You may modify the function signature and add other functionality as required. <br/>\n",
    "Now, consider the trigrams `\"alice said nothing\"` and `\"alice said nichts\"`. For these trigrams, estimate the values mentioned in the table given below and fill in the obtained results. The discounting parameter *d* = 0.75. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ja0jVjrh2zu8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V=  2749\n",
      "{'Nw1_w2_w3': 2, 'Nw1_w2': 11, 'Nplus_dot_w2_w3': 5, 'Nplus_dot_w2_dot': 319, 'Nplus_dot_w3': 23, 'Nplus_dot_dot': 14713, 'Nplus_w1_w2_dot': 6, 'Nplus_w2_dot': 56, 'lambda_w1_w2': 0.40909090909090906, 'lambda_w2': 0.09090909090909091}\n",
      "\n",
      "\n",
      "V=  2749\n",
      "{'Nw1_w2_w3': 0, 'Nw1_w2': 11, 'Nplus_dot_w2_w3': 0, 'Nplus_dot_w2_dot': 319, 'Nplus_dot_w3': 0, 'Nplus_dot_dot': 14713, 'Nplus_w1_w2_dot': 6, 'Nplus_w2_dot': 56, 'lambda_w1_w2': 0.40909090909090906, 'lambda_w2': 0.09090909090909091}\n"
     ]
    }
   ],
   "source": [
    "KN_model = exercise_2.KneserNey(tokens, d=0.75, N=3)\n",
    "\n",
    "t1 = \"alice said nothing\"\n",
    "t2 = \"alice said nichts\"\n",
    "\n",
    "# TODO\n",
    "# Get the required parameters\n",
    "KN_model.global_counter(tokens)\n",
    "\n",
    "params_t1 = KN_model.get_params(t1)\n",
    "print(params_t1)\n",
    "print('\\n')\n",
    "params_t2 = KN_model.get_params(t2)\n",
    "print(params_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4WFNsNc5YTq"
   },
   "source": [
    "| Term in Kneser-Ney |   Value t1  | Value t2 | Description | \n",
    "|---|---|---| --- |\n",
    "|$N(w_1w_2w_3)$|2  |0  |number of trigrams of the <br> combination $w_1 w_2 w_3$ |\n",
    "|$N(w_1 w_2)$| 11 |11  | number of bigrams of the <br> combination $w_1 w_2$|\n",
    "|$N_{+}( \\bullet w_2 w_3)$|5  |0  |number of words that precede <br> $w_2 w_3$ at least once in the corpus |\n",
    "|$N_{+}( \\bullet w_2 \\bullet)$|319  |319  |number of trigrams that precede $w_2$<br> and also has history $w_2$|\n",
    "|$N_{+}( \\bullet w_3)$|23  | 0 |unique number of words that precede <br> $w_3$ at least once in the corpus |\n",
    "|$N_{+}( \\bullet \\bullet)$|14713  |14713  |all bigrams appearing <br> atleast once |\n",
    "|$N_{+}(w_1 w_2 \\bullet)$|6 |6 |number of words observed<br> after $w_1 w_2$ atleast once|\n",
    "|$N_{+}(w_2 \\bullet)$|56  | 56 | number of words observed<br> after $w_2$ atleast once|\n",
    "|$\\lambda(w_1 w_2)$|0.409 |0.409 |scaling factor for bigram $w_1 w_2$ |\n",
    "|$\\lambda(w_2)$|0.090 |0.090 | scaling factor for unigram $w_1$|\n",
    "\n",
    "* Using the values obtained above, manually calculate $P_{KN}(w_3)$, $P_{KN}(w_3|w_2)$, and $P_{KN}(w_3|w_1, w_2)$ for the given trigrams. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers** <br>\n",
    "\\begin{align*}\n",
    "P_{KN}(w_3=\\text{nothing}) &= 23/14713 = 0.001563 \\\\\n",
    "P_{KN}(w_3=\\text{nothing}|w_2=\\text{said}) &= \\frac{max(5-0.75, 0)}{319} + 0.09090 \\cdot 0.001563 = 0.01346496 \\\\\n",
    "P_{KN}(w_3=\\text{nothing}|w_1=\\text{alice}, w_2=\\text{said}) &= \\frac{max(2-0.75, 0)}{11} + 0.40909 \\cdot 0.01346496 = 0.1191447 \\\\\n",
    "\\\\\n",
    "P_{KN}(w3=\\text{nichts}) &= 1/2749 = 0.000363 \\\\\n",
    "P_{KN}(w_3=\\text{nichts}|w_2=\\text{said}) &= \\frac{max(0-0.75, 0)}{319} + 0.09090 \\cdot 0.000363 = 3.29e-05 \\\\\n",
    "P_{KN}(w_3=\\text{nichts}|w_1=\\text{alice}, w_2=\\text{said}) &= \\frac{max(0-0.75, 0)}{11} + 0.40909 \\cdot 3.29e-05 = 1.34e-05\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4WFNsNc5YTq"
   },
   "source": [
    "**2.2 (0.5 points)**\n",
    "\n",
    "Take a look at this [video](https://www.youtube.com/watch?v=cbAxvpBFyNU) on Kneser-Ney smoothing by Dan Jurafksy. Make sure to undestand his *San Francisco* example. <br/>\n",
    "How will Kneser-Ney Smoothing handle the following bigrams (answer in 3-4 sentences)? \n",
    "\n",
    "* Abu Dhabi\n",
    "\n",
    "* Game Over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "In the bigram `Abu Dhabi`, the word `Dhabi` appears often but only after `Abu` while in `Start Over`, the word `Over` doesnt appear only after `Over` but also after many other possible words like `pull`, `talk`,`stop` etc. So, the unigram probablity should only be based on number of different words it follows. Therefore, the continuation probablity for `Game over` would be greater than `Abu Dhabi`. \n",
    "\n",
    "Also, the quantity $P_{KN}(w_i|w_{i-1})$ \n",
    "$$ \\frac{C(w_i=\\text{Dhabi},w_{i-1}=\\text{Abu})}{C(w_i=\\text{Abu})}  > \\frac{C(w_i=\\text{Over},w_{i-1}=\\text{Game})}{C(w_i=\\text{Game})} $$\n",
    "Since counts of `Dhabi` would be almost same as counts of `Abu Dhabi` while counts of `Game` would be greater than `Game Over`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pq7Yc1CmKdLi"
   },
   "source": [
    "## Bonus (2 points)\n",
    "\n",
    "For each of the smoothing techniques below,\n",
    "\n",
    "1. Laplace/add-1 smoothing (0.3 points)\n",
    "2. Add-$\\alpha$ smoothing (0.3 points)\n",
    "3. Linear interpolation (0.3 points)\n",
    "4. Absolute discounting (0.3 points)\n",
    "5. Good-Turing (0.3 points)\n",
    "6. Kneser-Ney smoothing (0.3 point)\n",
    "\n",
    "* Give the intuition behind it\n",
    "* State at least one drawback and\n",
    "* Explain how the ensuing smoothing technique accounts for this drawback.\n",
    "\n",
    "You can do so in continuous text or in bullet points. Write 3-5 sentences for each technique. For Kneser-Ney smoothing, you should suggest *and explain* an improved version from the literature, e.g. [here](http://nrs.harvard.edu/urn-3:HUL.InstRepos:25104739) (this tutorial may also be helpful for the rest of the exercise).\n",
    "\n",
    "Please note that while the points for this bonus exercise are the immediate motivation, your self-made comparison will be highly beneficial for the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \n",
    "- In order to deal with the 0 probability of an n-gram, we pretend to see each word an extra number of times. So we add 1 to all counts\n",
    "- It sometimes assigns too much probability mass from available ngrams in training data to ngrams absent in training data\n",
    "- Since adding 1 to all counts leads to the probability mass getting assigned too much (to absent ngrams), how about we control this number. We can change to a float value less than 1.\n",
    "\n",
    "2. \n",
    "- In add $\\alpha$ smoothing, we fix the problem of 0 probability by adding $\\alpha<1$ to each count. So the formula becomes:\n",
    "$$ P_{+\\alpha}(w_i|w_{i-1}) = \\frac{C(w_{i-1},w_i)+\\alpha}{C(w_{i-1})+\\alpha \\cdot V}$$\n",
    "- Even if we optimize $\\alpha$ to a suitable optimized value, this smoothing technique makes pretty bad predictions for word sequences.\n",
    "- Solution? we can try a different approach. until now we have been using counts of unigrams and bigrams to compute the conditional probability of bigrams and trigrams respectively. We cant use e.g. trigrams if we have very low evidence of it. Idea? What if we use all of the unigrams, bigrams, and trigrams. Let's see this approach.\n",
    "\n",
    "3. \n",
    "- Linear interpolation suggests that we use evidences from all of the unigrams, bigrams, and trigrams and multiply a factor ($\\lambda_1$, $\\lambda_2$, $\\lambda_3$) to each of them to control their properties that are favorable for our language model.\n",
    "- The optimal $\\lambda_i$ depends on the context but we cant tune all lambdas separately. We will need to bucket them.\n",
    "- \n",
    "\n",
    "4. \n",
    "- In absolute discounting  a constant value $ m$ is subtracted from each count. The effect of this is that the ngrams with the lowest counts are discounted relatively more than those with higher counts.\n",
    "- The problem with this method is that if we haven't seen a bigram at all. We are going to reply only on the unigram probability.\n",
    "- Good turing addresses this problem \n",
    "\n",
    "5. \n",
    "- The intitution behind Good-Turning estimation is to use the counts of ngrams we have seen once to estimate the counts of ngrams we have never seen.\n",
    "$$\n",
    "c^*_k = (c_k+1)\\frac{N_{k+1}}{N_k}\n",
    "$$\n",
    "- when $k$ is very large i.e. its values is equal to maximum occurrence in the training corpus (for eg. $k$=3 and there are no items are that appear $k+1$ times), then there is a possible conflict with the defined formula ($N_{k+1}=0$). We also have to make sure that the probabilities are always normalized\n",
    "\n",
    "6. \n",
    "- It is a modified version of absolute discounting. In this, we optimize the calculation of the lower-order n-gram probabilities in case the higher-order ngram was unseen in the corpus\n",
    "- The given article suggests not to use the same $d$ discounting parameter for all non-zero counts. It writes that we can have three different $D_1$, $D_2$, $D_{3+}$ that can be applied to unigrams, bigrams, and trigrams respectively"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
