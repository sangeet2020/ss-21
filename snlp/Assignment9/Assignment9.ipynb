{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xwZKbE3vFwP"
   },
   "source": [
    "# Assignment 9\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python files. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after your upload your submission, otherwise the assignment will not be considered as submitted. Only one from the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v6OVU_HvH9k"
   },
   "source": [
    "# Exercise 1: Text Classification (10 points)\n",
    "\n",
    "Based on your implementation of the `Corpus` and `Document` classes from the previous assignment, you will now build a simple Naive Bayes classifier to classify each document in the test section of the Reuters News corpus. \n",
    "\n",
    "We will use the TF-IDF metric as the feature for our classifier. TF-IDF of a term $t$ in a document $d$ is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{TF-IDF}(t,d) = \\frac{\n",
    "    \\text{TF}(t,d)\n",
    "  }{\n",
    "    \\text{IDF}(t)\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "with $\\text{TF}(t,d)$ being the defined as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{TF}(t,d) = \\frac{\n",
    "    f_{t,d}\n",
    "  }{\n",
    "    \\sum_{t'} f_{t',d}\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "where $f_{t,d}$ is the absolute frequency of term $t$ in document $d$.\n",
    "\n",
    "and $\\text{IDF}(t)$ being defined as \n",
    "\n",
    "\\begin{equation}\n",
    "  \\text{IDF}(t) = \\frac{\n",
    "    N\n",
    "  }{\n",
    "    |\\{d \\in D: C_d(t) > 0\\}|\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "where $D$ stands for the documents in the corpus, $N=|D|$ and $C_d(t)$ is the number of times term $t$ occurs in document $d$.\n",
    "\n",
    "In a TF-IDF matrix, documents are represented by the rows of the matrix and TF-IDF features by its columns. This means that each row vector consists of the TF-IDF value for a term taken from a fixed, shared vocabulary given the document, i. e. $\\text{TF-IDF}(t,d)$, for $t \\in V$ ([this](https://www.researchgate.net/profile/Maryam-Hourali/publication/306358542/figure/tbl1/AS:648973966651395@1531738859631/Some-Part-of-TF-IDF-Term-Document-Matrix.png) is a small example). \n",
    "\n",
    "## 1.1 Vocabulary as feature space (2 points)\n",
    "\n",
    "Construct a shared vocabulary $V$ for the Reuters corpus, using both the train set and the test set. You are expected to reduce the size of the vocabulary by \n",
    "  * Preprocessing (removing punctuation, lowercasing, tokenizing). (0.25 points)\n",
    "  * Lemmatizing the tokenized text. (0.5 points)\n",
    "  * Setting a $\\text{min_df}$ and $\\text{max_df}$ and removing all terms from the vocabulary that occur in less then $\\text{min_df}$ and more than $\\text{max_df}$ documents. You should support your choice with a source from the internet or your own reasoning. (0.5 point)\n",
    "  * Why is it necessary to reduce the size of the vocabulary and to set a lower and upper bound to document frequency? Explain in 2-3 sentences. (0.25 points)\n",
    "\n",
    "You are allowed to use any Python package useful to the task. We suggest using NLTK's [RegexpTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer) for tokenization and [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer) for lemmatization. The implementation should be in the `reduce_vocabulary` method of the `Corpus` class. Check that your implementation is correct by executing the code cell below and comparing vocabulary sizes before and after the reduction. \n",
    "  \n",
    "As always, you are free to define new methods as you need them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sYqeIW08JZg"
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "from nltk.corpus import reuters, stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FvJYIFZM6LQg"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_1\n",
    "exercise_1 = reload(exercise_1)\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading Reuters corpus...\")\n",
    "corpus = exercise_1.Corpus(\n",
    "    documents=[\n",
    "    exercise_1.Document(fileid, reuters.raw(fileid), reuters.categories(fileid), stop_words=stop_words) \n",
    "    for fileid in tqdm(reuters.fileids())],\n",
    "    categories=reuters.categories()\n",
    ")\n",
    "print(\"\\nVocab size before reduction:\", len(corpus.terms))\n",
    "\n",
    "# TODO: set min_df, max_df\n",
    "min_df = \n",
    "max_df = \n",
    "\n",
    "reduced_vocab = corpus.reduce_vocab(min_df=min_df, max_df=max_df)\n",
    "\n",
    "print(\"\\nVocab size after reduction:\", len(reduced_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ztKAsZ-HTV"
   },
   "source": [
    "## 1.2 TF-IDF matrix (2 points)\n",
    "\n",
    "1. Implement the method `_idfs` of the `Corpus` class. It should take the reduced vocabulary as input and return a dictionary containing the IDFs of each word in the reduced vocabulary. Print the IDFs of the first 10 terms (sorted lexicographically) from the reduced vocabulary. Store the IDFs in a class variable `idfs`. Why is it a good idea to calculate IDFs first? (1 points)\n",
    "\n",
    "2. Implement the method `_tfs_idfs` of the corpus class. It should return a vector or a a list containing the TF-IDFs of all terms in the reduced vocabulary for a single document. It should use the `_idfs` method once internally. (1 point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_gqWAqjjvFD-",
    "outputId": "b7be5b09-6cea-4498-fbab-a14c64434ee9"
   },
   "outputs": [],
   "source": [
    "# TODO: load and print IDFs!\n",
    "idfs = corpus.idfs(reduced_vocab)\n",
    "print(\"Estimating idfs\")\n",
    "for term in reduced_vocab[:10]:\n",
    "  print(term, idfs[term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z7TVYQyAgNW"
   },
   "source": [
    "## 1.3 Train/test split (1.5 points)\n",
    "\n",
    "1. Implement the method `_category2index`. It should take a string (the name of the category) as input and return its index in the `Corpus`-internal list of categories. (0.25 points)\n",
    "2. Implement the method `compile_dataset` of the `Corpus` class. It should take the reduced vocabulary as input and return two tuples: (train TF-IDF matrix, train labels) and (test TF-IDF matrix, test labels). The train matrix/labels should be derived from the train section of the Reuters dataset (file-ids starting with `training/`) and the test matrix/labels from the test section (file-ids starting with `test/`).\n",
    "\n",
    "  Make use of the methods `_tf_idfs` and `_category2index` (1 point)\n",
    "\n",
    "3. Use the method `compile_dataset` to load the train and test data into variables. Please name the variables such that we can distinguish the train data from the test data. Show the size of the train and test set. (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dv0gDKDWC2Dp"
   },
   "outputs": [],
   "source": [
    "# TODO: load train and test data\n",
    "(X_train, Y_train), (X_test, Y_test) = corpus.compile_dataset(reduced_vocab)\n",
    "\n",
    "# Show size of train and test set"
   ]
  },
  {
   "source": [
    "## 1.4: Naive Bayes Classifier (5 points)\n",
    "\n",
    "A Naive Bayes classifier assigns a datapoint $x = x_1,...x_n$ to a class $C_k$ ($1 \\leq k \\leq K$, with $K$ being the number of classes) with probability $P(C_k|x)$ given by:\n",
    "\n",
    "\\begin{equation}\n",
    "  p(C_k|x) = \\frac{\n",
    "    p(C_k)p(x|C_k)\n",
    "  }{\n",
    "    p(x)\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "1.  Describe the idea behind Naive Bayes in 3-4 sentences. Do so by explaining the terms 'naive' and 'Bayes(ian)' (1 point)\n",
    "\n",
    "**Answer:** Naive Bayes is based on Bayes' theorem (probabilistic), it decomposes the conditional probability in the following form : \n",
    "\n",
    "\\begin{equation}\n",
    "  p(C_k|x) = \\frac{\n",
    "    p(C_k)p(x|C_k)\n",
    "  }{\n",
    "    p(x)\n",
    "  }\n",
    "\\end{equation}\n",
    "\n",
    "The conditional probability for all classes $C_k$ is calculated and prediction is assigned to class with highest probability. There is also one more assumption that features in $x$ are statistically independent, which associate the term 'naive' to it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2. For each part of the above formula, assign it to one of the following categories, and give a short explanation. (1 point)\n",
    "  * Prior\n",
    "  * Posterior\n",
    "  * Likelihood\n",
    "  * Evidence\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "  * Prior - $p(C_k)$, how likely is the class overall\n",
    "  * Posterior - $p(C_k|x)$, conditional probability of class given the features\n",
    "  * Likelihood - $p(x|C_k)$, probability of certain features given the class\n",
    "  * Evidence - $p(x) = \\sum_{k} p(C_k)p(x|C_k)$, it sums over all classes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "3. In our dataset from 1.3, what corresponds to $C_k$? What to $x$? (0.5 points)\n",
    "\n",
    "**Answer:** $C_k$ corresponds to train labels obtained from method `_category2index`, and $x$ corresponds to train TF-IDF matrix obtained from methods `_tf_idfs`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC4liSllDOVQ"
   },
   "source": [
    "\n",
    "\n",
    "4. What is a good baseline for estimating the accuracy of our classifier? How would you evaluate it? Explain in 1-2 sentences **and** support your answer with code. This will also help you check the accuracy you get on the actual data. (1 point)\n",
    "\n",
    "5. Train a Naive Bayes classifier on the train section of our dataset and report precision, accuracy and F-score on the test section. You may use the class [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) and the method [precision_recall_fscore](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) from the [scikit-learn](https://scikit-learn.org/stable/install.html) Python package. You can write the code in the code cell below. (2 points)\n",
    "  \n",
    "6. Do you observe a difference in the F-scores of different classes? Why? What could you do to account for your finding? (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVMp0gIzIi0H"
   },
   "outputs": [],
   "source": [
    "# TODO: Find accuracy of baseline classifier\n",
    "\n",
    "# TODO: train classifier, report precision, recall, fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StlojTryeIdd"
   },
   "source": [
    "## Bonus: Support Vector Machines (1.5 points)\n",
    "\n",
    "Consider the task of Named Entity Recognition. In a simplified scenario, you want to decide for each word if it belongs to one of the following classes: {not-named-entity, person, city, country, currency}. An expert in the field tells you that you should start with the following set of features:\n",
    "- is the whole word in capitals\n",
    "- is the first letter capitalized\n",
    "- does it begin a sentence\n",
    "- number of characters \n",
    "- is a stopword\n",
    "- number of Wikipedia articles that contain this word in their title\n",
    "\n",
    "1. Come up with at least 3 more features for this problem. (0.2 points)\n",
    "2. How can we numerically represent each datapoint? What is the mathematical object called and what is the set in which it lives? (0.2 points)\n",
    "3. What is a hyperplane and how can it be used in this context? (0.2 points)\n",
    "4. Imagine that you've been given two features: $f_1, f_2$ and the following dataset. The task is currently only to distinguish between two classes. Draw the points and 3 hyperplanes:\n",
    "  - one that mispredicts at least one datapoint\n",
    "  - one that predicts everything correctly\n",
    "  - one that predicts everything correctly but is in some sense worse than the previous one\n",
    "\n",
    "|Data point|$f_1$|$f_2$|class|\n",
    "|---|---|---|---|\n",
    "|$d_1$|2|2|Y|\n",
    "|$d_2$|10|9|Y|\n",
    "|$d_3$|2|5|Y|\n",
    "|$d_4$|3|5|Y|\n",
    "|$d_5$|2|-2|N|\n",
    "|$d_6$|10|0|N|\n",
    "|$d_7$|10|-4|N|\n",
    "|$d_8$|3|3|N|\n",
    "\n",
    "  In all cases provide the formula for the hyperplane and explain how to use it to make a decision regarding which class it belongs to. (0.65 points)\n",
    "\n",
    "5. In the previous question, you created hyperplanes that helped you in determining which of the two classes the datapoint belongs to. How would you extend this to solve the original problem, i.e. predicting which of the 5 classes the datapoint belongs to? (0.25 points)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TH0UZYLl2Tp2",
    "K6a2UUP09Mh0"
   ],
   "name": "Assignment9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}