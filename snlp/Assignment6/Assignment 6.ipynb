{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7cXfz0cao0H"
   },
   "source": [
    "# Assignment 6\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python file for exercise 3. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after your upload your submission, otherwise the assignment will not be considered as submitted. Only one from the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yi9e8HHYbq7T"
   },
   "source": [
    "## Exercise 1: MLE & MAP  (3 points)\n",
    "\n",
    "Here is a nice [article](http://pages.cs.wisc.edu/~jerryzhu/cs838/LM.pdf) that explains the difference between MLE and MAP based estimation for language models. \n",
    "\n",
    "**1.1 (1 point)**\n",
    "\n",
    "* Is a MAP estimator always better than MLE? Why is MAP preferred over MLE (explain in context of language modelling)? (0.5 points)\n",
    "\n",
    "* Some smoothing methods use a MAP estimation of the model parameters. One of these is floor discounting, as described on Slide 44 in Chapter 5. <br/>\n",
    "Write the formula for deriving the MAP estimate and the resultant formula for floor discounting. What underlying distribution does the language model assume? What is assumed as the prior? (0.5 pts)\n",
    "\n",
    "**1.2 (2 points)**\n",
    "\n",
    "Consider the sentences\n",
    "\n",
    "`s1 = They don't know that we know they know!`\n",
    "\n",
    "`s2 = Don't we know that?`\n",
    "\n",
    "* Assume punctuation removal and lowercasing as basic preprocessing steps. Formulate a bigram model (use a circular corpus) on `s1`, note down the conditional probabilities, and use them to determine the probability of `s2`. (0.5 pt)\n",
    "* Now use floor discounting and form similar bigram models with $\\epsilon=1$ and $\\epsilon=0.5$, note down the respective bigram probabilities, and use them to determine the probability of `s2`. (1 pt) \n",
    "* What do you observe? How does the value of $\\epsilon$ influence the MAP prior in both cases? (0.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jjZwN7YbrMM"
   },
   "source": [
    "## Exercise 2: Good-Turing (3 points)\n",
    "\n",
    "#### 2.1: Estimate of unknown words (0.5 points)\n",
    "\n",
    "Imagine that you're given a split into (1) training data and (2) test data. They come from the same distribution but you're forbidden to make any other splits.\n",
    "\n",
    "- What is the MLE estimate of frequency of unseen words on (2)?\n",
    "- What other, better, strategy could be employed to estimate frequency of unseen words from (1) on (2)?\n",
    "\n",
    "#### 2.2: Data split (0.5 points)\n",
    "\n",
    "- Usually when preparing a new model, we split the dataset to (1) training data, (2) held-out data and (3) test data. Comment briefly on what each part of the dataset is used for and especially on the distinction between (1) and (2).\n",
    "\n",
    "#### 2.3: Good-Turing Formula (1 point)\n",
    "\n",
    "- What is the formula for updated counts for words of frequency $k \\ne 0$?\n",
    "- What is the Good-Turing formula for estimating unseen words, $k=0$?\n",
    "- Given the updated counts $c^*_k$, how does Good-Turing estimate relate to language model smoothing? How do you define the smoothed distribution?\n",
    "- Is there any systematic relationship between the original counts and the new counts? e.g. for large enough $k$, $c^*_k \\ge k$. \n",
    "- Compute and compare the absolute frequencies and Good-Turing estimates for the words `three` and `syringes` in the following text.\n",
    "\n",
    "```\n",
    "thirty - three thousand three hundred and thirty silver syringes which cost three hundred and thirty - two crowns . .\n",
    "```\n",
    "\n",
    "#### 2.4: Frequent buckets (1 point)\n",
    "\n",
    "- In your own words, what is the issue with Good-Turing when estimating the new counts for the very frequent words?\n",
    "- See page 5, paragraph 3 of [Good‚ÄêTuring Smoothing Without Tears](http://deanfoster.net/teaching/data_mining/good_turing.pdf) by Gale and Sampson (1995). What solution do they propose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HztKiKEpbrXt"
   },
   "source": [
    "## Exercise 3: Cross-Validation (4 points)\n",
    "\n",
    "<!-- ### 3.0 Bible studies -->\n",
    "\n",
    "Imagine you are a linguist in the remote future who just rediscovered a book called \"Bible\". To your disappointment the book is obviously incomplete; all the pages between Genesis and the Apocalypse are torn out, maybe by some late Christian cult. Since you don't know the language of the book you want to build a first language model that you can use if you find any of the lost parts. You digitize the book with your state-of-the art portable digitizer, and then load it into one of your Python 10.0 notebooks.\n",
    "\n",
    "**3.1 Baseline (0.5 points)**\n",
    "\n",
    "* The two corpora are in the text files `genesis.txt` and `apocalypsis.txt`. Load them into the notebook, preprocess them by removing all non-alphabetical characters, and then concatenate them into a single corpus. Split the corpus into a train and a test set, with the test set comprising the _last_ 20% of the corpus. You may use your code from previous exercises for this. (0.25 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDVzYp5zKSVC"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "from importlib import reload\n",
    "import exercise_3\n",
    "exercise_3 = reload(exercise_3)\n",
    "\n",
    "genesis_text = Path(\"data/genesis.txt\").open('r').read()\n",
    "apocalypsis_text = Path(\"data/apocalypsis.txt\").open('r').read()\n",
    "\n",
    "# TODO: preprocess\n",
    "genesis_preprocessed = \n",
    "apocalypsis_preprocessd =\n",
    "\n",
    "# TODO: concatenate\n",
    "corpus = \n",
    "\n",
    "# TODO: train, test split\n",
    "train, test ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk5YNPJ_Ky0s"
   },
   "source": [
    "* Using your language model class from the previous assignment, estimate a trigram language model on the train set and report perplexity on the test. Use $\\alpha=1$. Does this represent an unbiased estimate of the model's capacity? (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NsFQMfyPcl1"
   },
   "outputs": [],
   "source": [
    "# TODO: trigram LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9uIbyb7LoYT"
   },
   "source": [
    "**3.2 Average Perplexity (2 points)**\n",
    "\n",
    "* Since you want to get an unbiased estimate of your model's capacity, you decide to apply k-fold cross-validation on your corpus. To do this, implement the function `k_validation_folds` in `exercise_2.py`. Use it to split your corpus into $k=5$ cross-validation folds, and make sure that the folds are of the same size. (1 point)\n",
    "\n",
    "* Now, estimate a trigram language model on each of the CV folds. You may again use your class from the previous assignment, and average over all perplexity scores. Does the average score differ from the one obtained in 3.2, and why? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d2o5fTiMo13"
   },
   "outputs": [],
   "source": [
    "# 10-fold cross-validation\n",
    "cv_folds = exercise_3.k_validation_folds(corpus, k_folds=5)\n",
    "\n",
    "# TODO: estimate 10 trigram LMs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRakHZmFNALS"
   },
   "source": [
    "**3.3 Hyperparameter Tuning (1.5 points)**\n",
    "\n",
    "* As you don't know anything about the language the book is written in, you have to find the best hyperparemter $\\alpha$ for your model by a brute-force search (recall how you did this in the last assignment). Since you know from 3.2 that your data is not balanced, you decide to use only the averaged perplexity score (derived from $k=10$ CV folds) for this. Do so by completing the loop in the code cell below. Then, plot the obtained perplexity scores vs. $\\alpha$. You do so by reusing the respective function from Assignment 5. (1 point)\n",
    "\n",
    "(**Hint:** This could take a while.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ada-400O8kO"
   },
   "outputs": [],
   "source": [
    "# hyperparamter tuning, CV\n",
    "alphas = [x*0.01 for x in range(1,101)]\n",
    "\n",
    "for alpha in alphas:\n",
    "  # TODO: estimate LMs!\n",
    "\n",
    "# TODO: plot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGq72SKNP-tb"
   },
   "source": [
    "* Repeat the tuning process for unigram and bigram language models. Does your estimate of $\\alpha$ differ? Why? (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4as-q5FJP9wL"
   },
   "outputs": [],
   "source": [
    "# hyperparamter tuning, bigram + trigram\n",
    "\n",
    "for alpha in alphas:\n",
    "  # TODO: estimate LMs!\n",
    "\n",
    "# TODO: plot!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
