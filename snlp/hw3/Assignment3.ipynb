{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgfjrzxW3l1W"
   },
   "source": [
    "# SNLP Assignment 3\n",
    "\n",
    "Name 1: Sangeet Sagar<br/>\n",
    "Student id 1: 7009050<br/>\n",
    "Email 1: sasa00001@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: Nikhil Paliwal<br/>\n",
    "Student id 2: 7009915<br/>\n",
    "Email 2: nipa00002@stud.uni-saarland.de<br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for exercises 2 and 3. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y0Looy74Lor"
   },
   "source": [
    "## Exercise 1: Entropy Intuition (2 points)\n",
    "\n",
    "### 1.1 (0.5 points)\n",
    "\n",
    "Order the following three snippets by entropy (highest to lowest). Justify your answer (view it more intuitively rather than by using a specific character-level language model, though you would probably reach the same conclusion).\n",
    "\n",
    "```\n",
    "1:    A B A A A A B B A A A B A B B B B B A\n",
    "2:    A B A B A B A B A B A B A B A B A B A\n",
    "3:    A B A A A B A B A B A B A B A B A B A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "Correct order: $2 > 3 > 1$ <br>\n",
    "*Explanation*: Looking intuitively, Entropy is a measure of randomness in a probability distribution. We compare the entorpy in the above sequences by comparing radomness. **2** has the highest degre of randomness for the reason that no consecutive letters are same. This is followed by **3** as it has a repition of `A A A` in the beginning. This is observation is more prevelant in **1**, hence it has least entropy among all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y0Looy74Lor"
   },
   "source": [
    "### 1.2 (0.5 point)\n",
    "\n",
    "Words in natural language do not have the maximum entropy given the available alphabet. This creates a redundancy (e.g. the word `maximum` could be uniquely replaced by `mxmm` and everyone would still understand). If the development of natural languages leads to somewhat optimal solutions, why is it beneficial to have such redundancies in communication?\n",
    "\n",
    "If you're uncertain, please refer to this well-written article: [www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html](http://www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "Having redundancies diminishes the uncertainty in communication. With more information on the goal of communication, more certain we become what the speaker refers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y0Looy74Lor"
   },
   "source": [
    "### 1.3 (1 point)\n",
    "\n",
    "1. Assume you were given a perfect language model that would always assign probability of $1$ to the next word. What would be the cross-entropy on any text? Motivate your answer with formal derivation. (0.5 points)\n",
    "2. How does cross-entropy relate to perplexity? Is there a reason why would one be preferred over the other? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "Cross-entropy\n",
    "$$ H(P, Q) = \\quad –\\sum_{x \\in X} P(x) * \\log(Q(x)) $$\n",
    "1. In the given situation, $P(x) = Q(x) = 1$. Hence,\n",
    "$$ H(P, Q) = \\quad –\\sum_{x \\in X} 1 * \\log(1) $$\n",
    "$$ H(P, Q) = 0 $$\n",
    "Intuitively, if the probablity of the next word is 1, we are always certain about the subsequent outcomes, hence there is no un-certainity and the Entropy is $0$.\n",
    "\n",
    "2. Perplexity($M$) is given as $M = 2^{H(P, Q)}$. Hence it is equivalent to the exponentiation of the cross-entropy. Generally, perplexity is preferred over cross-entropy as it is easy to interpret (for the reason that perplexity is the avergae de-facto size of vocabluary).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VG5StIq4MVW"
   },
   "source": [
    "## Exercise 2: Harry Potter and the Measure of Uncertainty (4 points)\n",
    "\n",
    "#### 2.1 (2.5 points)\n",
    "\n",
    "Harry, Hermione, and Ron are trying to save the Philosopher's Stone. To do this, they have to cross a series of hurdles to reach the room where the stone is kept. Currently, they are trapped in a chamber whose exit is blocked by fire. On a table before them are 7 potions.\n",
    "\n",
    "|P1|P2|P3|P4|P5|P6|P7|\n",
    "|---|---|---|---|---|---|---|\n",
    "\n",
    "Of these, 6 potions are poisons and only one is the antidote that will get them through the exit. Drinking the poison will not kill them, but will weaken them considerably. \n",
    "\n",
    "1. There is no way of knowing which potion is a poison and which an antidote. How many potions must they sample *on an average* to pick the antidote? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "We have $X$ = no. of potion sampled before picking up an antidote. <br>\n",
    "\n",
    "$P(X=1) = \\frac{1}{7} \\quad \\quad \\quad$ (antidote is picked up in the first sampling) <br>\n",
    "$P(X=2) = \\frac{6}{7}\\cdot\\frac{1}{6} = \\frac{1}{7}\\quad$ (1 poison, 1 antidote) <br>\n",
    "$P(X=3) = \\frac{6}{7} \\cdot \\frac{5}{6} \\cdot \\frac{1}{5} = \\frac{1}{7} $ (2 poison, 1 antidote) <br>\n",
    "Similarly, <br>\n",
    "$P(X=4)= P(X=5)= P(X=6)= P(X=7) =\\frac{1}{7} $\n",
    "\n",
    "\n",
    "$$ E[x] = \\sum_{n=1}^{7} n\\cdot\\left(\\frac{1}{7}\\right)$$\n",
    "$$ E[x] = \\frac{1}{7}\\sum_{n=1}^{7} n$$\n",
    "$$ E[x] = 4$$\n",
    "\n",
    "Therefore, we must take sample 4 potions on an avergage to pick the antidote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VG5StIq4MVW"
   },
   "source": [
    "Hermione notices a scroll lying near the potions. The scroll contains an intricate riddle written by Professor Snape that will help them determine which potion is the antidote. With the help of the clues provided, Hermione cleverly deduces that each potion can be the antidote with a certain probability. \n",
    "\n",
    "|P1|P2|P3|P4|P5|P6|P7|\n",
    "|---|---|---|---|---|---|---|\n",
    "|1/16|1/4|1/64|1/2|1/64|1/32|1/8|\n",
    "\n",
    "2. In this situation, how many potions must they now sample *on an average* to pick the antidote correctly? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VG5StIq4MVW"
   },
   "source": [
    "3. What is the most efficient sequence of potions they must sample to discover the antidote? Why do you claim that in terms of how uncertain you are about guessing right? (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "**P4 > P2 > P7 > P1 > P6 > {P3, P4}** <br>\n",
    "The sequence of potion sampling given above diminishes the uncertainity (to maximum extent) as we take the potion with highest probablity of being an antidote at first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VG5StIq4MVW"
   },
   "source": [
    "#### 2.2 (1.5 points)\n",
    "\n",
    "1. Extend your logic from 2.1 to a Shannon's Game where you have to correctly guess the next word in a sentence. Assume that a word is any possible permutation and combination of 26 letters of the alphabet, and all the words have a length of at most *n*. \n",
    "How many guesses will one have to make to guess the correct word? (1 point) <br/>\n",
    "(**Hint**: Think of how many words can exist in this scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "<!-- Let $k$ be the total number of words of length atmost $n$ be present in the corpus:\n",
    "$$ k = \\sum_{n=1}^{26} 26^{n}$$\n",
    "Sum of a GP <br>\n",
    "$$ k = \\frac{26}{25}(26^{n}-1)$$\n",
    "\n",
    "Using similar logic from 2.1:\n",
    "$$ E[x] = \\sum_{m=1}^{k} m\\cdot\\left(\\frac{k-1}{k}\\right)^{m-1} \\cdot \\left(\\frac{1}{k}\\right)$$\n",
    "$$ E[x] = \\left(\\frac{1}{k}\\right) \\sum_{m=1}^{k} m\\cdot\\left(\\frac{k-1}{k}\\right)^{m-1}$$\n",
    "\n",
    "Let $ S =  \\sum_{m=1}^{k} m\\cdot\\left(\\frac{k-1}{k}\\right)^{m-1} $\n",
    "\\begin{align}\n",
    "S &= 1\\cdot\\left(\\frac{k-1}{k}\\right)^{0} + 2\\cdot\\left(\\frac{k-1}{k}\\right)^{1} + \\dots k\\cdot\\left(\\frac{k-1}{k}\\right)^{k-1} \\\\\n",
    "\\left(\\frac{k-1}{k}\\right)S &= 1\\cdot\\left(\\frac{k-1}{k}\\right)^{1} + 2\\cdot\\left(\\frac{k-1}{k}\\right)^{2} + \\dots (k-1)\\cdot\\left(\\frac{k-1}{k}\\right)^{k-1} + k\\cdot\\left(\\frac{k-1}{k}\\right)^{k}\n",
    "\\end{align}\n",
    "\n",
    "Subtracting above equations:\n",
    "$$ S\\cdot\\frac{1}{k} = 1 + \\sum_{m=2}^{k} \\left(\\frac{k-1}{k}\\right)^{m-1} - k\\cdot \\left(\\frac{k-1}{k}\\right)^{m-1} $$ -->\n",
    "\n",
    "Let $k$ be the total number of words of length atmost $n$ be present in the corpus:\n",
    "$$ k = \\sum_{n=1}^{26} 26^{n}$$\n",
    "\n",
    "Sum of a GP <br>\n",
    "$$ k = \\frac{26}{25}(26^{n}-1)$$\n",
    "\n",
    "Using similar logic form 2.1, we have <br>\n",
    "$E[X=1] = \\frac{1}{k} $ ; (expectation of a correct guess in the 1st sampling) <br>\n",
    "$E[X=2] = \\frac{k-1}{k} \\frac{1}{k-1} = \\frac{1}{k}$ ; (expectation of a correct guess in the 2nd sampling)<br>\n",
    "And so on, <br>\n",
    "$E[X=k] = \\frac{1}{k}$ ; (we sample as many times as we have totat number of words in the corpus)\n",
    "\n",
    "$$ E[x] = \\sum_{m=1}^{k} m\\cdot\\left(\\frac{1}{k}\\right)$$\n",
    "$$ E[x] = \\frac{1}{k}\\sum_{m=1}^{k} m$$\n",
    "$$ E[x] = \\frac{1}{k}\\cdot \\frac{k(k+1)}{2}$$\n",
    "\n",
    "Therefore, one has to make $\\frac{1}{k}\\cdot \\frac{k(k+1)}{2}$ (where $k = \\sum_{n=1}^{26} 26^{n}$) guesses to to guess the correct word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VG5StIq4MVW"
   },
   "source": [
    "2. Why is the entropy lower in real-world languages? How do language models help to reduce the uncertainty of guessing the correct word? (2-3 sentences) (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "Entropy is lower for real-world languages for the reason that the language is defined under a particular set of rules (e.g. Grammatical rules) and is confined to follow them. We as a speaker or a writer of the language follow a specific sentence structure. <br>\n",
    "A statistical language model is learned from raw text and predicts the probability of the next word in the sequence given the words already present in the sequence. Hence, given a word the LM will have certain choices to choose\n",
    "from to make the next prediction and it will select the one with maximum probablity, thus reducing the uncertainity of guessing the correct word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bx1tnPQ4MgW"
   },
   "source": [
    "## Exercise 3: Kullback-Leibler Divergence (4 points)\n",
    "\n",
    "Another metric (besides perplexity and cross-entropy) to compare two probability distributions is the Kullback-Leibler Divergence $D_{KL}$. It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $P$ is the empirical or observed distribution, and Q is the estimated distribution over a common probabilitiy space $X$. \n",
    "Answer the following questions:\n",
    "\n",
    "#### 3.1. (0.5 points)\n",
    "\n",
    "How is $D_{KL}$ related to Cross-Entropy? Derive a mathematical expression that describes the relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "$$ D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)} $$ \n",
    "$$ D_{KL}(P\\|Q) = -\\sum_{x \\in X}P(x)\\cdot \\log(Q(x)) + \\sum_{x \\in X}P(x)\\cdot \\log(Q(x)) $$ \n",
    "$$ D_{KL}(P\\|Q) = E_P[-\\log(Q)]- E_P[-\\log(P)]$$ \n",
    "$$ D_{KL}(P\\|Q) = H(P, Q)- H(P)$$ \n",
    "\n",
    "Where: <br>\n",
    "$ H(P,Q)$ = cross entropy of distributions $P$ and $Q$ <br>\n",
    "$ H(P)$ = entropy of distribution $P$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bx1tnPQ4MgW"
   },
   "source": [
    "#### 3.2. (0.5 points)\n",
    "\n",
    "Is minimizing $D_{KL}$ the same thing as minimizing Cross-Entropy?  Support your answer using your answer to 1.\n",
    "\n",
    "<!-- 3.3. Is $D_{KL}$ a distance metric, i. e. does $D_{KL}(P\\|Q) = D_{KL}(Q\\|P)$ hold? Justify you explanation by a proof or by a numerical counterexample. (1 point) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "Yes, minimizing cross-entrpy is same as minimizing $D_{KL}$ becuase entropy remains unchanged for a true distribution. Changes in the distribution are reflected only in the cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bx1tnPQ4MgW"
   },
   "source": [
    "#### 3.3 (3 points)\n",
    "\n",
    "For a function $d$ to be considered a distance metric, the following three properties must hold:\n",
    "\n",
    "$\\forall x,y,z \\in U:$\n",
    "\n",
    "1. $d(x,y) = 0 \\Leftrightarrow x = y$\n",
    "2. $d(x,y) = d(y,x)$\n",
    "3. $d(x,z) \\le d(x,y) + d(y,z)$\n",
    "\n",
    "Is $D_{KL}$ a distance metric? ($U$ in this case is the set of all distributions over the same possible states).\n",
    "For each of the three points either prove that it holds for $K_{DL}$ or show a counterexample proving why it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** <br>\n",
    "1. Let $x=p$, $y=q$ \n",
    "$$ D(p \\|q) = H(p, q) - H(p) \\quad \\quad \\quad  \\quad \\dots (1)$$\n",
    "$$ D(p \\|q) = H(p,p) - H(p)$$\n",
    "$$ D(p \\|q) = H(p) - H(p)$$\n",
    "$$ D(p \\|q) = 0$$\n",
    "$D_{KL}$ holds here.\n",
    "\n",
    "2. $$ D(q \\|p) = H(q, p) - H(q) \\quad \\quad \\quad  \\quad \\dots (2)$$\n",
    "From 1 and 2, \n",
    "$$D(q \\|p) \\neq D(p \\|q)$$\n",
    "$D_{KL}$ does not hold here. <br>\n",
    "Counterexample:\n",
    "\\begin{align}\n",
    "D(x\\|y) &= \\frac{1}{3} \\log\\left(\\frac{1/3}{1/6}\\right) + \\frac{2}{3} \\log\\left(\\frac{2/3}{5/6}\\right) \\\\\n",
    "D(x\\|y) &= 0.035 \\\\\n",
    "\\\\\n",
    "D(y\\|x) &= \\frac{1}{6} \\log\\left(\\frac{1/6}{1/3}\\right) + \\frac{5}{6} \\log\\left(\\frac{5/6}{2/3}\\right) \\\\\n",
    "D(y\\|x) &= 0.03 \\\\\n",
    "\\end{align}\n",
    "Hence\n",
    "$$D(x \\|y) \\neq D(y \\|x) $$\n",
    "\n",
    "3. $D_{KL}$ does not hold here. <br>\n",
    "Counterexample:\n",
    "Sample space : ${0, 1}$ <br>\n",
    "$x(0) = \\frac{1}{3}$ <br>\n",
    "$y(0) = \\frac{1}{6}$<br>\n",
    "$z(0) = \\frac{1}{12}$ <br>\n",
    "\n",
    "$$ D(p\\|q) = \\sum p_i \\log\\left(\\frac{p_i}{q_i}\\right) $$\n",
    "\\begin{align}\n",
    "D(x\\|z) &= \\sum x_i \\log\\left(\\frac{x_i}{z_i}\\right) \\\\\n",
    "D(x\\|z) &= x(0) \\log\\left(\\frac{x(0)}{z(0)}\\right) + x(1) \\log\\left(\\frac{x(1)}{z(1)}\\right) \\\\\n",
    "D(x\\|z) &= \\frac{1}{3} \\log\\left(\\frac{1/3}{1/12}\\right) + \\frac{2}{3} \\log\\left(\\frac{2/3}{11/12}\\right) \\\\\n",
    "D(x\\|z) &= 0.108 \\\\\n",
    "\\\\\n",
    "D(x\\|y) &= \\frac{1}{3} \\log\\left(\\frac{1/3}{1/6}\\right) + \\frac{2}{3} \\log\\left(\\frac{2/3}{5/6}\\right) \\\\\n",
    "D(x\\|y) &= 0.035 \\\\\n",
    "\\\\\n",
    "D(y\\|z) &= \\frac{1}{6} \\log\\left(\\frac{1/6}{1/12}\\right) + \\frac{5}{6} \\log\\left(\\frac{5/6}{1/12}\\right) \\\\\n",
    "D(x\\|z) &= 0.015 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Hence,\n",
    "$$D(x\\|z) \\ge D(x\\|y) + D(x\\|y)$$\n",
    "\n",
    "Therefore, $D_{KL}$ is not a distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8zkUP3l4Mxw"
   },
   "source": [
    "## Bonus (1.5 points)\n",
    "\n",
    "1. Compute $D_{KL}(Q_1\\|P_1)$ for the following pair of sentences based on a unigram language model (word level).\n",
    "\n",
    "```\n",
    "p1: to be or not to be\n",
    "q1: to be or to be or not or to be be be\n",
    "```\n",
    "\n",
    " Do so by implementing the function `dkl` in `bonus.py`. You will also have to calculate the distributions $P_1$, $Q_1$; for this, you can either reuse your code from the last assignment or implement a new function in `bonus.py`. (1 point)\n",
    "\n",
    "2. Suppose the sentences in 1. would be replaced by the following sequences of symbols. You can imagine them to be sequences of nucleobases in a [coding](https://en.wikipedia.org/wiki/Coding_region) region of a gene in your genome.\n",
    "\n",
    "```\n",
    "p2: ACTGACACTGAC\n",
    "q2: ACTACTGACCCACTACTGACCC\n",
    "```\n",
    "\n",
    "Let $P_2$, $Q_2$ be the character-level unigram LMs derived from these sequences. What values will $D_{KL}(P_1\\|P_2)$, $D_{KL}(Q_1\\|Q_2)$ take? Does the quantity hold any information? Would computing $D_{KL}$ between distributions over two different natural languages hold any information? (0.5 points)\n",
    "\n",
    "No mathematical explanation nor coding required for the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fBOuBNr6FY8"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import bonus\n",
    "bonus = reload(bonus)\n",
    "\n",
    "# TODO: estimate LMs\n",
    "P = \n",
    "Q = \n",
    "\n",
    "# TODO: DKL\n",
    "print(bonus.dkl(p,q))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
