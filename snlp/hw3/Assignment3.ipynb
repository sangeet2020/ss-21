{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgfjrzxW3l1W"
   },
   "source": [
    "# SNLP Assignment 3\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python file for the bonus question (if you attempt it). <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y0Looy74Lor"
   },
   "source": [
    "## Exercise 1: Entropy Intuition (2 points)\n",
    "\n",
    "### 1.1 (0.5 points)\n",
    "\n",
    "Order the following three snippets by entropy (highest to lowest). Justify your answer (view it more intuitively rather than by using a specific character-level language model, though you would probably reach the same conclusion).\n",
    "\n",
    "```\n",
    "1:    A B A A A A B B A A A B A B B B B B A\n",
    "2:    A B A B A B A B A B A B A B A B A B A\n",
    "3:    A B A A A B A B A B A B A B A B A B A\n",
    "```\n",
    "\n",
    "### 1.2 (0.5 point)\n",
    "\n",
    "Words in natural language do not have the maximum entropy given the available alphabet. This creates a redundancy (e.g. the word `maximum` could be uniquely replaced by `mxmm` and everyone would still understand). If the development of natural languages leads to somewhat optimal solutions, why is it beneficial to have such redundancies in communication?\n",
    "\n",
    "If you're uncertain, please refer to this well-written article: [www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html](http://www-math.ucdenver.edu/~wcherowi/courses/m5410/m5410lc1.html).\n",
    "\n",
    "### 1.3 (1 point)\n",
    "\n",
    "1. Assume you were given a perfect language model that would always assign probability of $1$ to the next word. What would be the cross-entropy on any text? Motivate your answer with formal derivation. (0.5 points)\n",
    "2. How does cross-entropy relate to perplexity? Is there a reason why would one be preferred over the other? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VG5StIq4MVW"
   },
   "source": [
    "## Exercise 2: Harry Potter and the Measure of Uncertainty (4 points)\n",
    "\n",
    "#### 2.1 (2.5 points)\n",
    "\n",
    "Harry, Hermione, and Ron are trying to save the Philosopher's Stone. To do this, they have to cross a series of hurdles to reach the room where the stone is kept. Currently, they are trapped in a chamber whose exit is blocked by fire. On a table before them are 7 potions.\n",
    "\n",
    "|P1|P2|P3|P4|P5|P6|P7|\n",
    "|---|---|---|---|---|---|---|\n",
    "\n",
    "Of these, 6 potions are poisons and only one is the antidote that will get them through the exit. Drinking the poison will not kill them, but will weaken them considerably. \n",
    "\n",
    "1. There is no way of knowing which potion is a poison and which an antidote. How many potions must they sample *on an average* to pick the antidote? (1 point)\n",
    "\n",
    "Hermione notices a scroll lying near the potions. The scroll contains an intricate riddle written by Professor Snape that will help them determine which potion is the antidote. With the help of the clues provided, Hermione cleverly deduces that each potion can be the antidote with a certain probability. \n",
    "\n",
    "|P1|P2|P3|P4|P5|P6|P7|\n",
    "|---|---|---|---|---|---|---|\n",
    "|1/16|1/4|1/64|1/2|1/64|1/32|1/8|\n",
    "\n",
    "2. In this situation, how many potions must they now sample *on an average* to pick the antidote correctly? (1 point)\n",
    "3. What is the most efficient sequence of potions they must sample to discover the antidote? Why do you claim that in terms of how uncertain you are about guessing right? (0.5 point)\n",
    "\n",
    "#### 2.2 (1.5 points)\n",
    "\n",
    "1. Extend your logic from 2.1 to a Shannon's Game where you have to correctly guess the next word in a sentence. Assume that a word is any possible permutation and combination of 26 letters of the alphabet, and all the words have a length of at most *n*. \n",
    "How many guesses will one have to make to guess the correct word? (1 point) <br/>\n",
    "(**Hint**: Think of how many words can exist in this scenario)\n",
    "\n",
    "2. Why is the entropy lower in real-world languages? How do language models help to reduce the uncertainty of guessing the correct word? (2-3 sentences) (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bx1tnPQ4MgW"
   },
   "source": [
    "## Exercise 3: Kullback-Leibler Divergence (4 points)\n",
    "\n",
    "Another metric (besides perplexity and cross-entropy) to compare two probability distributions is the Kullback-Leibler Divergence $D_{KL}$. It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $P$ is the empirical or observed distribution, and Q is the estimated distribution over a common probabilitiy space $X$. \n",
    "Answer the following questions:\n",
    "\n",
    "#### 3.1. (0.5 points)\n",
    "\n",
    "How is $D_{KL}$ related to Cross-Entropy? Derive a mathematical expression that describes the relationship. \n",
    "\n",
    "\n",
    "#### 3.2. (0.5 points)\n",
    "\n",
    "Is minimizing $D_{KL}$ the same thing as minimizing Cross-Entropy?  Support your answer using your answer to 1.\n",
    "\n",
    "<!-- 3.3. Is $D_{KL}$ a distance metric, i. e. does $D_{KL}(P\\|Q) = D_{KL}(Q\\|P)$ hold? Justify you explanation by a proof or by a numerical counterexample. (1 point) -->\n",
    "\n",
    "\n",
    "#### 3.3 (3 points)\n",
    "\n",
    "For a function $d$ to be considered a distance metric, the following three properties must hold:\n",
    "\n",
    "$\\forall x,y,z \\in U:$\n",
    "\n",
    "1. $d(x,y) = 0 \\Leftrightarrow x = y$\n",
    "2. $d(x,y) = d(y,x)$\n",
    "3. $d(x,z) \\le d(x,y) + d(y,z)$\n",
    "\n",
    "Is $D_{KL}$ a distance metric? ($U$ in this case is the set of all distributions over the same possible states).\n",
    "For each of the three points either prove that it holds for $K_{DL}$ or show a counterexample proving why it does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8zkUP3l4Mxw"
   },
   "source": [
    "## Bonus (1.5 points)\n",
    "\n",
    "1. Compute $D_{KL}(Q_1\\|P_1)$ for the following pair of sentences based on a unigram language model (word level).\n",
    "\n",
    "```\n",
    "p1: to be or not to be\n",
    "q1: to be or to be or not or to be be be\n",
    "```\n",
    "\n",
    " Do so by implementing the function `dkl` in `bonus.py`. You will also have to calculate the distributions $P_1$, $Q_1$; for this, you can either reuse your code from the last assignment or implement a new function in `bonus.py`. (1 point)\n",
    "\n",
    "2. Suppose the sentences in 1. would be replaced by the following sequences of symbols. You can imagine them to be sequences of nucleobases in a [coding](https://en.wikipedia.org/wiki/Coding_region) region of a gene in your genome.\n",
    "\n",
    "```\n",
    "p2: ACTGACACTGAC\n",
    "q2: ACTACTGACCCACTACTGACCC\n",
    "```\n",
    "\n",
    "Let $P_2$, $Q_2$ be the character-level unigram LMs derived from these sequences. What values will $D_{KL}(P_1\\|P_2)$, $D_{KL}(Q_1\\|Q_2)$ take? Does the quantity hold any information? Would computing $D_{KL}$ between distributions over two different natural languages hold any information? (0.5 points)\n",
    "\n",
    "No mathematical explanation nor coding required for the second part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fBOuBNr6FY8"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import bonus\n",
    "bonus = reload(bonus)\n",
    "\n",
    "# TODO: estimate LMs\n",
    "P = \n",
    "Q = \n",
    "\n",
    "# TODO: DKL\n",
    "print(bonus.dkl(p,q))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}