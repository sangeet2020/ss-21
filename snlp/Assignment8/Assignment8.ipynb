{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_9q0FnT64lO"
   },
   "source": [
    "# Assignment 8\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python files for exercise 1. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after your upload your submission, otherwise the assignment will not be considered as submitted. Only one from the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2rbN80LjDxe"
   },
   "source": [
    "## Exercise 1: Document Classification Warm-Up (4 points)\n",
    "\n",
    "In this exercise you will explore the Reuters News corpus and apply basic feature selection techniques to it. You can download the reuters corpus and English stop words via [NLTK](https://www.nltk.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlRx8i2UpOp8",
    "outputId": "6b971473-c017-45c4-d1a6-2b2f0c7cdb5e"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ro0rb5UJpnbV"
   },
   "source": [
    "It consists of articles from [Reuters](https://de.wikipedia.org/wiki/Reuters) mapped to one of \n",
    "Files and categories (classes) can be accessed easily. Although there is a train/test split, we will use the whole corpus for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoVUEdpHprE0",
    "outputId": "57656f6e-fc5b-4890-d5c0-f323395661cb"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters, stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(reuters.categories())\n",
    "print(\"# of categories: {}\".format(len(reuters.categories())))\n",
    "reuters.fileids()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuC62kI3r1Lx"
   },
   "source": [
    "Each document belongs to one or more categories. For the sake of simplicity we map each document to the first of its categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xb9sHFIsBCe",
    "outputId": "b37dbd3a-b962-4401-bd5f-b541b9debf15"
   },
   "outputs": [],
   "source": [
    "print(reuters.categories('test/14833'))\n",
    "category = reuters.categories('test/14833')[0]\n",
    "category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRhPGNRfrKm0"
   },
   "source": [
    "**1.1: Categories (1.5 points)**\n",
    "\n",
    "1. Load data into the `Corpus` class. We suggest to give an attribute `documents` that contains a list of `Document` objects. Preprocessing steps (tokenization, lowercasing, stopword removal) should happen within the `Document` objects. As always, you can deviate from that if you find it more convenient. (1 point)\n",
    "2. Implement the method `category_frequencies` in the `Corpus` class. It should calculate the absolute frequencies of each category in the whole corpus. (0.25 points)\n",
    "3. Plot categories vs. their frequency on a log-log scale. Do so by implementing the function `plot_category_frequencies`. What can you say about the distribution the category frequencies you observe? (0.25 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "4pT7Rjf3s3WV",
    "outputId": "ece1c5da-d21f-468a-9beb-81bc39c91cbe"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_1\n",
    "exercise_1 = reload(exercise_1)\n",
    "\n",
    "corpus = exercise_1.Corpus(\n",
    "    documents=[exercise_1.Document(fileid, reuters.raw(fileid), reuters.categories(fileid), stop_words=stop_words) for fileid in reuters.fileids()],\n",
    "    categories=reuters.categories()\n",
    ")\n",
    "\n",
    "exercise_1.plot_category_frequencies(corpus.category_frequencies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTcwwcZ9wJJj"
   },
   "source": [
    "**1.2 Document Frequency (1.5 points)**\n",
    "\n",
    "A term's document frequency (DF) is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "DF_{c_k}(t_i) = \n",
    "\\frac{\n",
    "\\text{# of times $t_i$ occurs in $C_k$}\n",
    "}{\n",
    "\\text{# of documents of category $C_k$}\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "1. Find the 10 most frequent terms of the `housing ` category. Do so by implementing the method `term_frequencies` in the `Corpus` class. (0.5 points)\n",
    "2. For these 10 terms, calculate their DF in the `housing` category as well as in the whole corpus (over all categories). Do so by implementing the method`df` in the `corpus` class. Plot the results by implementing the function `plot_dfs`. (0.5 points)\n",
    "3. Is DF a good predictor for a category? Explain in 2-3 sentences. (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "-Qk9ELGFyTQN",
    "outputId": "0f59775f-8e8a-41bf-cb92-c89ad0640ce8"
   },
   "outputs": [],
   "source": [
    "most_common_housing = corpus.term_frequencies('housing').most_common(10)\n",
    "most_common_housing = [term for term, _ in most_common_housing]\n",
    "\n",
    "dfs_housing = [corpus.df(term, category='housing') for term in most_common_housing]\n",
    "dfs_all = [corpus.df(term) for term in most_common_housing]\n",
    "\n",
    "exercise_1.plot_dfs(most_common_housing, dfs_housing)\n",
    "exercise_1.plot_dfs(most_common_housing, dfs_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LfVk3Cjtxpy"
   },
   "source": [
    "**1.3 Pointwise Mutual Information (1 point)**\n",
    "\n",
    "PMI is given by:\n",
    "\\begin{equation}\n",
    "PMI(x;y) = \\log_2 \\frac{p(x,y)}{p(x)p(y)}\n",
    "\\end{equation}\n",
    "\n",
    "1. Give an interpretation of PMI. When will it be large, when will it be small? What are its upper and lower bound? (0.25 points)\n",
    "2. Calculate PMI of the `housing` category and each of the 10 most frequent terms from that category. Do so by implementing the method `pmi` in the `Corpus` class. Plot the PMI values. Do so by implementing the function `plot_pmis`. (0.5 points)\n",
    "3. What does the plot tell you about the suitability of the most frequent words of a category as a feature for text classification? Come up with an improved measure that makes use of both the document frequency and PMI (it need not be derived formally from PMI, but it should be motivated by your findings). (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "90hCYHNEvBse",
    "outputId": "2a78141e-a4cc-4f70-8f4a-67aaf10977db"
   },
   "outputs": [],
   "source": [
    "pmis = [corpus.pmi('housing', term) for term in most_common_housing]\n",
    "exercise_1.plot_pmis('housing', most_common_housing, pmis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQIEs2JpFkAB"
   },
   "source": [
    "## Exercise 2 $\\chi^2$ (2p)\n",
    "\n",
    "Study the usage of $\\chi^2$ statistics online. A very brief overview with examples can be found on [Wikipedia](https://en.wikipedia.org/wiki/Chi-squared_test).\n",
    "\n",
    "**2.1 $\\chi^2$ Introduction (1.25 points)**\n",
    "\n",
    "1. How can you turn a numerical feature into a categorical one? What are the parameters of this decision? What are some features (at least 3) one may encounter in NLP? (0.25 points)\n",
    "2. With the following knowledge of binary features, what's the expected count of $c(A \\cap B), c(\\neg A \\cap B), c(A \\cap \\neg  B), c(\\neg A \\cap \\neg B)$? \\\\\n",
    "Total count $N = 100$, $c(A) = 35, c(B) = 61$. (0.25 points)\n",
    "3. Using the given observations compute the $\\chi^2$ statistics and the respective probability of the features $B$ and $C$. (0.5 points)\n",
    "\n",
    "||A|$\\neg$ A|\n",
    "|-|-|-|\n",
    "|B|6|55|\n",
    "|$\\neg$ B|29|10|\n",
    "\n",
    "||A|$\\neg$ A|\n",
    "|-|-|-|\n",
    "|C|21|40|\n",
    "|$\\neg$ C|14|25|\n",
    "\n",
    "4. Based on your research of $\\chi^2$, what does the computed probability signify (in terms of hypotheses)? What is the hypothesis? Would you say that $B$ and $C$ are good features to decribe $A$? (0.25 points)\n",
    "\n",
    "\n",
    "**2.2 $\\chi^2$ Feature selection (0.75 points)**\n",
    "\n",
    "1. Given the set of categorical features $F$ and one specific target feature $f_0$, how can you use the $\\chi^2$ statistics to select the most predictive feature? (0.25 points)\n",
    "2. If you know all the features in $F$ are **not** pair-wise independent, how do you select $k$ features to describe $f_0$ together? (0.25 points)\n",
    "3. If you know all the features in $F$ are pair-wise independent, how do you select $k$ features to describe $f_0$ together? (Try to be computationally efficient and therefore provide a different algorithm than for the previous question.) (0.25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnDalRQcDY70"
   },
   "source": [
    "## Exercise 3: Authorship Identification (4 points)\n",
    "\n",
    "The science of *stylometry* assumes that authors have unconscious writing habits. Therefore, the words and grammar in a text become reliable indicators of the author of the text. Various research techniques have tried to do a feature-level analysis for the task of authorship identification/attribution. \n",
    "\n",
    "**3.1 (1 point)** \n",
    "\n",
    "In Chapter 6, you studied different types of classification. For the particular problem of authorship identification, suggest how you could formulate this task for each of the following categories (1 sentence each)\n",
    "\n",
    "- Binary classification\n",
    "- Multi-class classification\n",
    "- Flat classification\n",
    "- Hierarchical classification\n",
    "- Single category classification\n",
    "- Multi-category classification\n",
    "- Clustering\n",
    "\n",
    "**3.2 (2.5 points)**\n",
    "\n",
    "Read this [paper](https://dl.acm.org/doi/pdf/10.3115/1220355.1220443) which does a feature study for authorship identification. Answer the following questions (1-3 sentences each):\n",
    "\n",
    "1. What are the 6 categories of features extracted for the task? Explain in 1 sentence each. (0.5 points)\n",
    "2. What does the author observe in comparison to the baseline? (0.5 points) \n",
    "3. The author suggests using [SVM](https://en.wikipedia.org/wiki/Support-vector_machine)s as the classifier. What are the advantages offered by SVMs which guarantee a high accuracy in this problem? (0.5 point) \n",
    "4. The author concludes several possible scopes of improvement. Based on what you have studied so far, suggest if any improvements can be made in the given study w.r.t. (1 point)\n",
    "  - Feature selection \n",
    "  - Usage of n-gram features\n",
    "  - Any other approach other than the above two\n",
    "\n",
    "**3.3 (0.5 points)** \n",
    "\n",
    "State 3 real-life, contemporary applications where authorship identification/verification/attribution is useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP_lThfUDf9X"
   },
   "source": [
    "## Bonus (1.5 points) \n",
    "\n",
    "Consider the two plots given below. \n",
    "\n",
    "* For Plot 1 and Plot 2, are any or both of the features 1 and 2 \n",
    "1. redundant\n",
    "2. useful\n",
    "3. irrelevant\n",
    "\n",
    "    for a task like clustering? Explain your choice from the above 3. (1 point)\n",
    "    \n",
    "![Plots](features.png \"Plots 1 and 2\")\n",
    "\n",
    "* Draw/create a plot which shows features 1 and 2 as the remaining of the 3 choices stated above. Explain your reasoning in 1-2 sentences. (0.5 point)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
