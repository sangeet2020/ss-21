{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuLYCkGygMdT"
   },
   "source": [
    "# Assignment 7\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the Python files for exercises 1 and 2. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after your upload your submission, otherwise the assignment will not be considered as submitted. Only one from the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feU_fawmgoPp"
   },
   "source": [
    "## Exercise 1: Count-Trees (5 points)\n",
    "\n",
    "Your task is to implement a count-tree with variable maximum history. It is a memory-efficient way to store n-gram counts which can also be used to create an intuitive back-off after the tree is pruned.\n",
    "\n",
    "The tree object should support the following four operations:\n",
    "- Increment a count of a specific n-gram (even if not present)\n",
    "- Retrieve counts given the history (variable length)\n",
    "- Retrieve the conditional probability of a word given the history. (Proportion count between branches)\n",
    "- Pruning all nodes with counts less or equal to $k$\n",
    "\n",
    "**1.1 (2 points)**\n",
    "\n",
    "Make sure your implementation is correct by passing the asserts in the first code cell.\n",
    "\n",
    "**1.2 (1 point)**\n",
    "\n",
    "The next cell will incrementally add a quad-gram to the tree. Plot the perplexity of trigram language model (induced by this count tree) against the number of added n-grams. Comment on the curve shape. Smooth this language model with a zerogram distribution using a linear combination ($0.75\\times p_4 + 0.25\\times p_0$).\n",
    "\n",
    "**1.3 (1 point)**\n",
    "\n",
    "For the given range of thresholds, prune your tree and see how the threshold affects the performance. Plot the results (perplexity vs. threshold).\n",
    "\n",
    "**1.4 (1 point)**\n",
    "\n",
    "1. If you first prune with threshold $k_1$ and get tree $t_1$, then prune with $k_2$ and get $t_2$ what will be the relationship between $t_1$ and $t_2$ if $k_1 \\ge k_2$? (0.25 points)\n",
    "2. What is the memory benefit of count trees, in comparison to storing the counts as a dictionary `{n-gram:freq}`? (0.25 points)\n",
    "3. If we pruned the tree so that only the first level is preserved, what distribution could we model with this tree? (0.25 points)\n",
    "4. Pruning the count tree is said to be a dynamic way of smoothing the language model. Elaborate on how this smoothing happens. (0.25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_zKzeBzZLksw"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_1\n",
    "exercise_1 = reload(exercise_1)\n",
    "\n",
    "tree = exercise_1.CountTree(n=4)\n",
    "\n",
    "assert tree.get(\"\") == 0\n",
    "tree.add(\"ABCE\")\n",
    "tree.add(\"ABCD\")\n",
    "tree.add(\"ABCD\")\n",
    "tree.add(\"QBCD\")\n",
    "tree.add(\"QQCD\")\n",
    "tree.add(\"BCDA\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"1234\")\n",
    "tree.add(\"5634\")\n",
    "assert tree.get(\"ABCD\") == 2\n",
    "assert tree.get(\"ABCX\") == 0\n",
    "assert tree.get(\"BCD\") == 3\n",
    "assert tree.get(\"D\") == 4\n",
    "assert tree.get(\"CD\") == 4\n",
    "assert tree.get(\"1234\") == 5\n",
    "assert tree.get(\"5634\") == 1\n",
    "tree.prune(4)\n",
    "assert tree.get(\"ABCD\") == 4\n",
    "assert tree.get(\"XXCD\") == 4\n",
    "assert tree.get(\"D\") == 4\n",
    "assert tree.get(\"1234\") == 5\n",
    "assert tree.get(\"5634\") == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "mZQ7BgD3Xr2j",
    "outputId": "56c5573c-c789-4f1f-df4d-ba909b97fa6a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_x = []\n",
    "plot_y = []\n",
    "\n",
    "ngrams = []\n",
    "tree = exercise_1.CountTree(n=4)\n",
    "with open(\"data/alice_in_wonderland.txt\", \"r\") as f:\n",
    "  tokens = f.read().lower().split()\n",
    "  for i in range(len(tokens)-4):\n",
    "    ngrams.append(tokens[i:i+4])\n",
    "\n",
    "vocab = set(tokens)\n",
    "for i,ngram in enumerate(ngrams):\n",
    "  tree.add(ngram)\n",
    "  if i % 1000 == 0:\n",
    "    plot_x.append(i)\n",
    "    plot_y.append(tree.perplexity(ngrams, vocab))\n",
    "\n",
    "plt.plot(plot_x, plot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "QYPY_Whbooor",
    "outputId": "bb6c8e97-f6ea-41fd-e578-557fdb281ce1"
   },
   "outputs": [],
   "source": [
    "plot_x = []\n",
    "plot_y = []\n",
    "\n",
    "for threshold in [1,2,3,4,5,10,25,50,75,100]:\n",
    "  tree.prune(threshold)\n",
    "  plot_x.append(threshold)\n",
    "  plot_y.append(tree.perplexity(ngrams, vocab))\n",
    "\n",
    "print(plot_x)\n",
    "print(plot_y)\n",
    "\n",
    "plt.plot(plot_x, plot_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uC_TytVNlgtr"
   },
   "source": [
    "## Exercise 2: Kneser-Ney Smoothing (5 points)\n",
    "\n",
    "This exercise aims to provide a basic understanding of Kneser-Ney Smoothing. Kneser-Ney Smoothing makes use of *continuation counts* of words for lower order n-grams, given as\n",
    "\n",
    "\\begin{equation}\n",
    "C_{KN} = \n",
    "\\begin{cases}\n",
    "\\text{count}(\\bullet) & \\text{for highest order} \\\\\n",
    "\\text{continuationcount}(\\bullet) & \\text{for lower orders}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "For a trigram distribution, Kneser-Ney Smoothing is implemented using the following equations:\n",
    "\n",
    "$$P_{KN}(w_3|w_1, w_2) = \\frac{\\max\\{N(w_1 w_2 w_3)-d,0\\}}{N(w_1 w_2)} + \\lambda(w_1, w_2)P_{KN}(w_3|w_2)$$\n",
    "\n",
    "$$P_{KN}(w_3|w_2) = \\frac{\\max\\{N_{+}(\\bullet w_2 w_3)-d,0\\}}{N_{+}(\\bullet w_2 \\bullet)} + \\lambda(w_2)P_{KN}(w_3)$$\n",
    "\n",
    "\\begin{equation}\n",
    "P_{KN}(w_3) = \\begin{cases}\n",
    "\\frac{N_{+}(\\bullet w_3)}{N_{+}(\\bullet \\bullet)} & \\text{if $w_3 \\in$ V} \\\\\n",
    "\\frac{1}{V} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "$\\lambda$ is used to normalise the discounted probability mass and is given by\n",
    "\n",
    "$$\\lambda(w_1, w_2) = \\frac{d}{N(w_1 w_2)} \\cdot N_{+}(w_1 w_2 \\bullet)$$\n",
    "\n",
    "$$\\lambda(w_2) = \\frac{d}{N(w_2)} \\cdot N_{+}(w_2 \\bullet)$$\n",
    "\n",
    "**2.1 (4.5 points)**\n",
    "\n",
    "* Your first task is to understand what these terms represent and fill it in the table below (4-5 words each).\n",
    "\n",
    "* Create a trigram-level model on the given text, `alice_in_wonderland.txt`. Write your implementation in the file `exercise_2.py`. Preprocess the text by punctuation removal, lowercasing, and tokenisation. There is no need to split the data into train and test sets. (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vNH_zLxM5amO"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_2\n",
    "exercise_2 = reload(exercise_2)\n",
    "\n",
    "file = open(\"data/alice_in_wonderland.txt\", \"r\")\n",
    "text = file.read()\n",
    "\n",
    "# TODO: Preprocess text\n",
    "tokens = exercise_2.preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vw1xFixt2rUa"
   },
   "source": [
    "* \n",
    "Write a simple class `KneserNey` in `exercise_2` that calculates the different parameters required for finding the trigram conditional probability. You may modify the function signature and add other functionality as required. <br/>\n",
    "Now, consider the trigrams `\"alice said nothing\"` and `\"alice said nichts\"`. For these trigrams, estimate the values mentioned in the table given below and fill in the obtained results. The discounting parameter *d* = 0.75. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ja0jVjrh2zu8"
   },
   "outputs": [],
   "source": [
    "KN_model = exercise_2.KneserNey(tokens, d=0.75, N=3)\n",
    "\n",
    "t1 = \"alice said nothing\"\n",
    "t2 = \"alice said nichts\"\n",
    "\n",
    "# TODO\n",
    "# Get the required parameters\n",
    "KN_model.get_params(t1)\n",
    "KN_model.get_params(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4WFNsNc5YTq"
   },
   "source": [
    "| Term in Kneser-Ney |   Value t1  | Value t2 | Description | \n",
    "|---|---|---| --- |\n",
    "|$N(w_1w_2w_3)$|  |  | |\n",
    "|$N(w_1 w_2)$|  |  | |\n",
    "|$N_{+}( \\bullet w_2 w_3)$|  |  | |\n",
    "|$N_{+}( \\bullet w_2 \\bullet)$|  |  | |\n",
    "|$N_{+}( \\bullet w_3)$|  |  | |\n",
    "|$N_{+}( \\bullet \\bullet)$|  |  | |\n",
    "|$N_{+}(w_1 w_2 \\bullet)$| | | |\n",
    "|$N_{+}(w_2 \\bullet)$|  |  | |\n",
    "|$\\lambda(w_1 w_2)$| | | |\n",
    "|$\\lambda(w_2)$| | | |\n",
    "\n",
    "* Using the values obtained above, manually calculate $P_{KN}(w_3)$, $P_{KN}(w_3|w_2)$, and $P_{KN}(w_3|w_1, w_2)$ for the given trigrams. (1 point)\n",
    "\n",
    "**2.2 (0.5 points)**\n",
    "\n",
    "Take a look at this [video](https://www.youtube.com/watch?v=cbAxvpBFyNU) on Kneser-Ney smoothing by Dan Jurafksy. Make sure to undestand his *San Francisco* example. <br/>\n",
    "How will Kneser-Ney Smoothing handle the following bigrams (answer in 3-4 sentences)? \n",
    "\n",
    "* Abu Dhabi\n",
    "\n",
    "* Game Over\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pq7Yc1CmKdLi"
   },
   "source": [
    "## Bonus (2 points)\n",
    "\n",
    "For each of the smoothing techniques below,\n",
    "\n",
    "1. Laplace/add-1 smoothing (0.3 points)\n",
    "2. Add-$\\alpha$ smoothing (0.3 points)\n",
    "3. Linear interpolation (0.3 points)\n",
    "4. Absolute discounting (0.3 points)\n",
    "5. Good-Turing (0.3 points)\n",
    "6. Kneser-Ney smoothing (0.3 point)\n",
    "\n",
    "* Give the intuition behind it\n",
    "* State at least one drawback and\n",
    "* Explain how the ensuing smoothing technique accounts for this drawback.\n",
    "\n",
    "You can do so in continuous text or in bullet points. Write 3-5 sentences for each technique. For Kneser-Ney smoothing, you should suggest *and explain* an improved version from the literature, e.g. [here](http://nrs.harvard.edu/urn-3:HUL.InstRepos:25104739) (this tutorial may also be helpful for the rest of the exercise).\n",
    "\n",
    "Please note that while the points for this bonus exercise are the immediate motivation, your self-made comparison will be highly beneficial for the exam."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}