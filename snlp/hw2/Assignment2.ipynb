{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXSB0ZZUmrLd"
   },
   "source": [
    "# SNLP Assignment 2\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for exercises 2 and 3. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IQh2t-LF1uz"
   },
   "source": [
    "## Exercise 1 (1.5 + 1.5 = 3 points)\n",
    "\n",
    "The perplexity of a model can also be defined as $2^{-\\frac{1}{n} \\sum^n_1 \\log p(w_i|w_{i-1})}$. For the following exercise, use the log probabilities given this pretrained bigram language model. Tokenization is apparent from the tokens in the following table.\n",
    "\n",
    "|A|B|log p(B\\|A)|\n",
    "|-|-|-|\n",
    "|`The`|`man`|-1.8|\n",
    "|`the`|`man`|-2.2|\n",
    "|`the`|`post`|-2.7|\n",
    "|`Man`|`the`|-5.1|\n",
    "|`man`|`the`|-3.7|\n",
    "|`man`|`shouted`|-2.9|\n",
    "|`shouted`|`\"`|-3.1|\n",
    "|`post`|`!`|-3.1|\n",
    "|`\"`|`Man`|-1.9|\n",
    "|`\"`|`man`|-1.7|\n",
    "|`!`|`\"`|-1.2|\n",
    "|`\"`|`The`|-0.9|\n",
    "|`\"`|`the`|-1.2|\n",
    "\n",
    "Assume probabilities not listed are $0^+$ (and the respective logarithm $-\\infty$). For counting bigrams, consider your corpus as a circular structure i.e. include the bigram $(w_N, w_1)$ in your final counts. Therefore the weight of each bigram is $\\frac{1}{|\\text{words}|}$.\n",
    "\n",
    "### 1.1 Lowercasing Input (1.5 points)\n",
    "\n",
    "Compute the perplexity of the following two sentences (and show the steps).\n",
    "\n",
    "```\n",
    "The man shouted \"Man the post!\"\n",
    "the man shouted \"man the post!\"\n",
    "```\n",
    "\n",
    "Is lowercasing the input always a good idea? What are the advantages and disadvantages?\n",
    "\n",
    "### 1.2 Unknown Tokens (1.5 points)\n",
    "\n",
    "Compute the perplexity of the following two sentences.\n",
    "\n",
    "```\n",
    "The man shouted \"Man the stations!\"\n",
    "The man shouted \"Man the the!\"\n",
    "```\n",
    "\n",
    "Elaborate on the computed results. 2. Do you consider both sentences to be equally probable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3HUlC50F6uj"
   },
   "source": [
    "## Exercise 2 (N-gram models) (1 + 2 = 3 points)\n",
    "\n",
    "### 2.1\n",
    "\n",
    "Consider the formula on Page 28 in Chapter 2. \n",
    "\n",
    "$$P(w_2 | w_1) = \\frac{P(w_1,w_2)}{P(w_1)}$$\n",
    "\n",
    "To actually estimate these n-gram probabilities over a text corpus, we use **Maximum Likelihood Estimation (MLE)**. The estimate for the parameters of the MLE is obtained by getting counts from the corpus and then normalising them so they lie between 0 and 1.\n",
    "\n",
    "Using this, state the empirical formula for finding the conditional probability of unigrams $P(w)$, bigrams $P(w_2|w_1)$, and trigrams $P(w_3|w_1,w_2)$ for a corpus of N words. We do not expect any mathematical proof here, but just the formula for finding the conditional probabilities from the words in the corpus using the shown equation as the starting point. (1 pt)\n",
    "\n",
    "\n",
    "### 2.2 \n",
    "\n",
    "Given the corpus `orient_express.txt`, find the unigram, bigram, and trigram probability distributions of the text using the formulae obtained in 2.1. Implement the function `find_ngram_probs` in the file `exercise_2.py`. For counting bigrams and trigrams, consider your corpus as a circular structure i.e. include the bigram $(w_N, w_1)$ and trigrams $(w_{N-1}, w_N, w_1)$ and $(w_{N}, w_1, w_2)$ in your final counts.\n",
    "\n",
    "Using the probabilities you obtain, \n",
    "1. Plot the probabilities of the 20 most frequent unigrams \n",
    "2. For the most frequent unigram, plot the 20 most frequent bigrams starting with that unigram\n",
    "3. For the most frequent bigram, plot the 20 most frequent trigrams starting with that bigram\n",
    "\n",
    "Use the function `plot_most_frequent`. Briefly explain your observations (1-2 lines).\n",
    "\n",
    "NOTE: You must preprocess the text (remove punctuation, special characters, lowercase, tokenise) before you create your n-gram model. **You are NOT allowed to use nltk or any other tokeniser for this purpose**. Write your own function called `preprocess` in `exercise_2.py`. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tOFieJxFn3lo"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_2\n",
    "exercise_2 = reload(exercise_2)\n",
    "\n",
    "file = open(\"data/orient_express.txt\", \"r\")\n",
    "text = file.read()\n",
    "\n",
    "# TODO: Preprocess text\n",
    "tokens = exercise_2.preprocess(text)\n",
    "\n",
    "# TODO: Find conditional probabilities of unigrams, bigrams, trigrams\n",
    "\"\"\"\n",
    "Modify your function call based on how you have defined find_ngram_probs \n",
    "in exercise_2.py\n",
    "\"\"\"\n",
    "unigrams = exercise_2.find_ngram_probs(tokens, model='unigram')\n",
    "bigrams = exercise_2.find_ngram_probs(tokens, model='bigram')\n",
    "trigrams = exercise_2.find_ngram_probs(tokens, model='trigram')\n",
    "\n",
    "# TODO: Plot most frequent ngrams\n",
    "\"\"\"\n",
    "Modify the function signature as per your definition of plot_most_frequent \n",
    "in exercise_2.py\n",
    "\"\"\"\n",
    "exercise_2.plot_most_frequent(unigrams)\n",
    "exercise_2.plot_most_frequent(bigrams)\n",
    "exercise_2.plot_most_frequent(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vTjntEhF6UE"
   },
   "source": [
    "## Exercise 3 (4 points)\n",
    "\n",
    "### 3.1 \n",
    "\n",
    "Read the corpus file again and apply the preprocessing steps from Exercise 2. Split the corpus into a train and test sections; the size of the test section should be 10% of the corpus. Do this by implementing the `train_test_split` function in `exercise_3.py`.  Then, train 3-, 2- and 1-gram language models with your implementation from Exercise 2 on the train section. You may change the parameters of the functions if you find it necessary, but the code should still be written in the .py file. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u09wtYPAB928"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import exercise_3, exercise_2\n",
    "exercise_3 = reload(exercise_3)\n",
    "exercise_2 = reload(exercise_2)   \n",
    "\n",
    "file = Path(\"../data/orient_express.txt\").open('r')\n",
    "text = file.read()\n",
    "\n",
    "# TODO: apply tokenizer from exercise 2\n",
    "tokenized = exercise_2.preprocess(text)\n",
    "\n",
    "# TODO: split the corpus into a train corpus and a test corpus, with test_size=10%\n",
    "train, test = exercise_3.train_test_split(tokenized, 0.1)\n",
    "\n",
    "# TODO: train unigram, bigram, trigram LM using the method defined in exercise_2\n",
    "# call each method as per your function definition\n",
    "\n",
    "unigram_lm = exercise_2.find_ngram_probs(train, model='unigram')\n",
    "bigram_lm = exercise_2.find_ngram_probs(train, model='bigram')\n",
    "trigram_lm = exercise_2.find_ngram_probs(train, model='trigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJTD84VFB_LS"
   },
   "source": [
    "### 3.2\n",
    "\n",
    "Calculate relative frequencies for all three test corpora. Do this by implementing the function `relative_frequencies` in `exercise_3.py`. <br/>\n",
    "Relative frequency is calculated as follows: <br/>\n",
    "e. g. for bigrams, $ f(w_{i-1}, w_i) = \\frac{N(w_{i-1}, w_i)}{N(\\bullet,\\bullet)}$, where $N( w_{i-1},w_i)$ is the count of the bigram and $N(\\bullet,\\bullet)$ is the total number of bigrams in the corpus. For consistency, you should include a bigram $(w_N, w_1)$, where $N$ is the length of the corpus (and likewise for trigrams) as you have done in 2.2. (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b013xCFKB-VM"
   },
   "outputs": [],
   "source": [
    "# TODO: calculate unigram, bigram, trigram relative frequencies\n",
    "unigram_rfs = exercise_3.relative_frequencies(test)\n",
    "bigram_rfs = exercise_3.relative_frequencies(test, model='bigram')\n",
    "trigram_rfs = exercise_3.relative_frequencies(test, model='trigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFxe81afB_7f"
   },
   "source": [
    "### 3.3\n",
    "\n",
    "Implement the perplexity calculation for all 3 language models in the function `pp`, and perform the calculation on the test section of the corpus. You should use the perplexity formula from slide 21, chapter 3:\n",
    "\\begin{equation}\n",
    "  PP = 2^{-\\sum_{w,h}f(w,h)\\log_2 P(w|h)}\n",
    "\\end{equation}\n",
    "\n",
    "* Can you simply apply the formula to the language model and the relative frequencies? What would happen if an ngram from the test set is absent in the train set?\n",
    "\n",
    "* Why is it possible to calculate perplexity with this formula? How does it differ from the formula in exercise 1 of this sheet? \n",
    "\n",
    "(1.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJPplv-zB-ak"
   },
   "outputs": [],
   "source": [
    "# \"Smoothing\"\n",
    "unigram_rfs = {unigram:rf for unigram, rf in unigram_rfs.items() if unigram in unigram_lm}\n",
    "bigram_rfs = {bigram:rf for bigram, rf in bigram_rfs.items() if bigram in bigram_lm}\n",
    "trigram_rfs = {trigram:rf for trigram, rf in trigram_rfs.items() if trigram in trigram_lm}\n",
    "\n",
    "# TODO: compute perplexity for each LM\n",
    "unigram_pp = exercise_3.pp(unigram_lm, unigram_rfs)\n",
    "bigram_pp = exercise_3.pp(bigram_lm, bigram_rfs)\n",
    "trigram_pp = exercise_3.pp(trigram_lm, trigram_rfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgspP4f4CGqq"
   },
   "source": [
    "### 3.4 \n",
    "\n",
    "Plot perplexity scores for all 3 language models. Do so by implementing the `plot_pps` function.\n",
    "* Explain the differences between the language models. \n",
    "* Is it always a good idea to increase the history for n-gram based language models? What can happen if n is too large? (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHFKLCM1CGyl"
   },
   "outputs": [],
   "source": [
    "# TODO: plot\n",
    "pps = [unigram_pp, bigram_pp, trigram_pp]\n",
    "exercise_3.plot_pps(pps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtAvYxzMF608"
   },
   "source": [
    "## Bonus (1.5 points)\n",
    "\n",
    "Revisit exercise 1.\n",
    "\n",
    "1. Come up with another metric (not language model) as an alternative to perplexity that could measure language model capabilities.\n",
    "2. What are the advantages and disadvantages of such a metric in comparison to perplexity?\n",
    "3. Compute your metric with respect to the four sentences (in exercise 1) and the provided language model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "o8Q2HjmEF6OL"
   ],
   "name": "Assignment2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}