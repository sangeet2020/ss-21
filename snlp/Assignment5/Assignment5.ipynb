{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omz65i0aWLy-"
   },
   "source": [
    "# Assignment 5\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for exercises 1 and 2. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder in Teams. Make sure to click on \"Turn-in\" after your upload your submission, otherwise the assignment will not be considered as submitted. Only one from the group should make the submisssion.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrDBECH7WUsn"
   },
   "source": [
    "## Exercise 1: Out Of Vocabulary Words (3.5 points)\n",
    "\n",
    "As you saw in the lecture, the higher the number of unseen tokens in your language corpus, the higher the OOV rate. In this exercise, you will calculate the OOV rate for different languages for different vocabulary sizes. For each corpus, preprocess the data by lowercasing the text and applying tokenisation. Since there isn't any standard tokeniser that will work on all the languages, we recommend that you write your own function called `preprocess` in `exercise_1.py`.\n",
    "\n",
    "### 1.1 Preprocess data (0.5 points)\n",
    "\n",
    "Preprocess the data and partition it in a 70-30% train-test split. For this, write your own function `train_test_split_data` in `exercise_1.py`. You may modify the function signature and the code in the cell below appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGPQCH_UhXQQ"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import os\n",
    "import exercise_1\n",
    "exercise_1 = reload(exercise_1)\n",
    "\n",
    "# Walk through the data directory and read all the corpora\n",
    "# For each corpus, read the text, preprocess it and create the train test split for each language\n",
    "\n",
    "corpora = {} # To save the respective corpora\n",
    "\n",
    "# TODO: Add a loop over each file\n",
    "pp = exercise_1.preprocess(text) #TODO: preprocess text\n",
    "train, test = exercise_1.train_test_split_data(pp, test_size=) #TODO: split data\n",
    "\n",
    "#TODO: Add respective splits to the corpora dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh_XId7ThofH"
   },
   "source": [
    "### 1.2 Calculate OOV rates (1.5 points)\n",
    "For every language, construct a vocabulary by taking the 15000 most frequent tokens in the training set. Compute the OOV rate for vocabulary sizes 1k, 2k, ..., 15k. Implement this in the function `get_oov_rates` in `exercise_1.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhVdBujShuLQ"
   },
   "outputs": [],
   "source": [
    "oov_rates = {}\n",
    "for lang, (train, test) in corpora.items():\n",
    "    oov_rates[lang] = exercise_1.get_oov_rates(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr7n00qzhzqC"
   },
   "source": [
    "### 1.3 Plotting OOV rates (1 point) \n",
    "* Using the loglog scale, plot the OOV rate against the vocabulary size for all the languages in a single plot. Make sure your legend identifies the languages appropriately and you label the axes.\n",
    "\n",
    "* Describe your observations in 3-4 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KALigsBsh_kK"
   },
   "outputs": [],
   "source": [
    "exercise_1.plot_oov_rates(oov_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrQT4braiDYY"
   },
   "source": [
    "### 1.4 Handling OOV words (0.5 points)\n",
    "* Before applying smoothing and backing-off models, we need to take care of the OOV words. Suggest 2 techniques to handle Out-Of-Vocabulary words your corpus.\n",
    "* What are the advantages and disadvantages of each?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjv3vTKCWjZD"
   },
   "source": [
    "## Exercise 2: Smoothing (4 points)\n",
    " \n",
    "### 2.1 Additive smoothing (1 point)\n",
    "\n",
    "In the last assignments we largely ignored the issue of unseen words, i. s. words that are not in the train set/observed data but part of the test set. A very simple method to account for unseen words is [additive smoothing](https://en.wikipedia.org/wiki/Additive_smoothing). It assigns a small 'pseudo-count' to all unseen words AND to the words already in the language model, and then uses the updated counts to estimate the n-gram probabilities. The formula for unigram probabilities is:\n",
    "\\begin{equation}\n",
    "p(w_i) = \\frac{C(w_i) + \\alpha}{N + \\alpha |V|}\n",
    "\\end{equation}\n",
    "\n",
    "Where\n",
    "* $C(w_i)$ is the empirical count of the unigram $w_i$\n",
    "* $N$ is the number of unigrams in the train set\n",
    "* $|V|$ is the size of the vocabulary after smoothing\n",
    "* $\\alpha$ is the additive count.\n",
    "\n",
    "If $\\alpha = 1$ this is known as *Laplace* smoothing, if $0 < \\alpha < 1$ *Lidstone* smoothing.\n",
    "\n",
    "1. How would you estimate the bigram probability $p(w_i|w_{i-1})$ and the general case $p(w_i|w_{i-1}, ..., w_{n-i+1})$? Explain each part of the formula. (0.5 points)\n",
    "2. Is it a good idea to set $\\alpha$ to 1? What could be a more reasonable value, and why? (0.5 points)\n",
    "\n",
    "### 2.2 Language model class (3 points)\n",
    "\n",
    "Until now, you have implemented language models as a series of Python functions. We have provided to you a class skeleton in `lm.py` that should do all the tricks you need to estimate a language model. You will use the same corpora and train/test split as in Exercise 1.\n",
    "\n",
    "1. Complete the implementation of the `LanguageModel` class. You may estimate the parameters of the language model as you like, but the method `perplexity` should perform the perplexity calculation (as in the below code block), and the method `lidstone_smoothing` should smooth the data. You may define new methods or change the signatures of existing ones, as long as you comment on your changes. Make sure that the relative frequencies and the conditional probabilities for each history sum up to 1. (1.5 points)\n",
    "\n",
    "2. Choose $\\alpha = 1$. Then, estimate $N = 1,2,3$ language models for the corpora from Exercise 1, and plot perplexity vs. $n$ for each of them. Do so by implementing the function `plot_pp` in `exercise_2.py` Do you observe any differences between the languages? Explain what you see in 3-4 sentences. (1 point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qAiMpNkau-cV"
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import lm\n",
    "import exercise_2\n",
    "lm = reload(lm)\n",
    "exercise_2 = reload(exercise_2)\n",
    "\n",
    "N = 3\n",
    "\n",
    "PPS = []\n",
    "\n",
    "for lang, (train, test) in corpora.items():\n",
    "  LM = lm.LanguageModel(train, test, N=N, alpha=1)\n",
    "  # TODO: calculate perplexity\n",
    "\n",
    "exercise_2.plot_pp(PPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMD8IOu9vAB7"
   },
   "source": [
    "3. Now, find a good value for $\\alpha$ for the *English* corpus. Do so by estimating $K=100$ trigram language models with $\\alpha = 0.0, 0.01,...,0.99,1.0$, and plot trigram perplexity vs. increasing $\\alpha$. You can write the code for the loop in the code cell below, the plotting code should be in `plot_pp_vs_alpha` in `exercise_2.py`. Does the $\\alpha$ coincide with your estimate in 2.1.2? (0.5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQW72tGUu-pr"
   },
   "outputs": [],
   "source": [
    "# only for 1 langauge?\n",
    "lang = \"corpus.en\"\n",
    "\n",
    "N = 3\n",
    "K = 100\n",
    "\n",
    "PPs = []\n",
    "\n",
    "# TODO: Loop\n",
    "\n",
    "exercise_2.plot_pp_vs_alpha(PPs, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI_F9ysnoqmy"
   },
   "source": [
    "\n",
    "## Exercise 3: Misc. (2.5 points)\n",
    "\n",
    "## 3.1 Smoothed perplexity (1 point)\n",
    "\n",
    "Assume you trained (MLE) an n-gram language model on datasets $D_\\text{train}$ and $D_\\text{test}$. You measure perplexities $p_{1,\\text{train}}$ and $p_{1,\\text{test}}$ respectively. You then smooth your n-gram language model and evaluate again on the two datasets, resulting in $p_{2,\\text{train}}$ and $p_{2,\\text{test}}$. Answer the following question with brief comments (e.g. _\"X is always greater than Y because ..\"_). For a language model $p$, test perplexity can, for example, be computed as $2^{\\frac{-1}{|D_\\text{test}|} \\sum_{w \\in D_\\text{test}}\\log p(w|h)}$ and train perplexity as $2^{\\frac{-1}{|D_\\text{train}|} \\sum_{w \\in D_\\text{train}}\\log p(w|h)}$.\n",
    "\n",
    "1. What is the relation of $p_{1,\\text{train}}$ and $p_{1,\\text{test}}$?\n",
    "2. What is the relation of $p_{2,\\text{train}}$ and $p_{2,\\text{test}}$?\n",
    "3. What is the relation of $p_{1,\\text{train}}$ and $p_{2,\\text{train}}$?\n",
    "4. What is the relation of $p_{1,\\text{test}}$ and $p_{2,\\text{test}}$?\n",
    "5. How does $n$ size affect the perplexities?\n",
    "\n",
    "## 3.2 Infinite smoothing (0.5 points)\n",
    "\n",
    "What distribution would you get if you applied additive or absolute discounting (choose one) smoothing infinitely? e.g. if $F_\\text{smooth}$ is a function that smooths a language model (either additive or absolute discounting) and $\\text{lm}^{(n+1)} = F_\\text{smooth}(\\text{lm}^{(n)})$. What will the language model $\\lim_{n\\rightarrow \\infty} \\text{lm}^{(n)}$ look similar to?\n",
    "\n",
    "## 3.3 Convex combination of LM models (1 point)\n",
    "\n",
    "Consider the following quantity based on two independent language models $p_1$ and $p_2$.\n",
    "\n",
    "$f_3(w|h) = \\beta_1\\cdot p_1(w|h) + \\beta_2\\cdot p_2(w|h)$ where $\\beta_1 + \\beta_2 = 1$ and $\\beta_1 \\ge 0, \\beta_2 \\ge 0$\n",
    "\n",
    "- Is it still a language model (probability distribution given history $h$)? Show that all properties hold or find a counterexample for each: (1) non-negativity, (2) summation to 1 and (3) $\\sigma$-additivity. See [Wikipedia - Probability Axioms](https://en.wikipedia.org/wiki/Probability_axioms). (0.5 points)\n",
    "- What would be the possible gain of using the given function as a language model? (0.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11It8uAJWqUT"
   },
   "source": [
    "## Bonus (1 point)\n",
    "\n",
    "Read about the following special language models techniques. The provided links shold only serve as a starting point.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. [Neural language models](https://en.wikipedia.org/wiki/Language_model#Neural_network) (0.5 points)\n",
    "\n",
    "- Describe (~5 sentences or bullet points) the working, advantages and disadvantages of NLM.\n",
    "\n",
    "#### 2. [Class-based language models](https://www.cs.cmu.edu/~roni/11761/PreviousYearsHandouts/classlm.pdf) (0.5 points)\n",
    "\n",
    "- What is a class-based language model?\n",
    "- What issues does it address?\n",
    "- Can we utilize the output for something more than just language modelling (think about other NLP problems and classes you took)?\n",
    "\n",
    "<!-- #### 2. [Decoding](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/) (0.5p)\n",
    "\n",
    "- What issues does beam search in the context of text generation using language models solve? -->\n",
    " <!-- and . Be specific and practical in your answers and support your arguments with evidence.  -->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 5",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
